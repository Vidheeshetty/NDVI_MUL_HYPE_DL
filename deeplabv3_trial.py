# -*- coding: utf-8 -*-
"""deeplabv3_trial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nOuWPui3lGiYo_2vbkj0AoSTajhsfK7K
"""

import os
import zipfile

# If using Colab with a zip on Drive:
from google.colab import drive
drive.mount('/content/drive')

zip_path = "/content/drive/MyDrive/Agri.zip"  # adjust as needed
output_dir = "/content/data_unzipped"

os.makedirs(output_dir, exist_ok=True)

with zipfile.ZipFile(zip_path, 'r') as zf:
    zf.extractall(output_dir)

print("Data unzipped to:", output_dir)
print("Top-level structure:", os.listdir(output_dir))

# Commented out IPython magic to ensure Python compatibility.
!apt-get install -y unzip
!pip install rasterio
!pip install torch torchvision  # Just to ensure we have PyTorch
!pip install scikit-learn      # for optional clustering if needed

import os
import glob
import numpy as np
import rasterio
from rasterio.plot import show
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

# %matplotlib inline

# Example: adapt to match your nested structure
deep_dir_path = os.path.join(
    output_dir,
    "Agri",
    "Paddy",
    "paddy_season4_RededgeMultispectral_20190917_05m",
    "000"
)

print("Deep directory path where .tif images are located:", deep_dir_path)

# Let's see what files are there
print("Files in deep_dir_path:", os.listdir(deep_dir_path))

import re
from collections import defaultdict

all_tifs = sorted([f for f in os.listdir(deep_dir_path) if f.endswith(".tif")])
print("Total TIF files found in deep_dir_path:", len(all_tifs))

# Group them by prefix (e.g., IMG_0009)
grouped_files = defaultdict(list)
pattern = r"(IMG_\d+)_(\d+)\.tif"  # e.g. "IMG_0009_1.tif"

for tif_file in all_tifs:
    match = re.match(pattern, tif_file)
    if match:
        img_id = match.group(1)    # e.g. "IMG_0009"
        grouped_files[img_id].append(tif_file)

# Sort each group's file list by the numeric suffix (1..5)
for img_id in grouped_files:
    grouped_files[img_id] = sorted(
        grouped_files[img_id],
        key=lambda x: int(re.match(pattern, x).group(2))
    )

print("Number of image sets:", len(grouped_files))

# Inspect first group
sample_key = list(grouped_files.keys())[0]
print("Example group:", sample_key, "->", grouped_files[sample_key])

##############################################
# 4. PREPROCESS: CROP, NDVI, SAVE
#    + PLOT A SAMPLE
##############################################
import numpy as np
import rasterio
import cv2
import matplotlib.pyplot as plt

def crop_black_borders(band_stack):
    """
    Crops out rows/cols where all bands are zero.
    band_stack: (bands, H, W)
    """
    non_zero_mask = np.any(band_stack != 0, axis=0)
    rows = np.any(non_zero_mask, axis=1)
    cols = np.any(non_zero_mask, axis=0)
    rmin, rmax = np.where(rows)[0][0], np.where(rows)[0][-1]
    cmin, cmax = np.where(cols)[0][0], np.where(cols)[0][-1]
    return band_stack[:, rmin:rmax+1, cmin:cmax+1]

def calculate_ndvi(band_stack, red_idx=2, nir_idx=4):
    """
    NDVI = (NIR - RED)/(NIR + RED)
    band order [B=0, G=1, R=2, RE=3, NIR=4]
    """
    red = band_stack[red_idx].astype(np.float32)
    nir = band_stack[nir_idx].astype(np.float32)
    with np.errstate(divide='ignore', invalid='ignore'):
        ndvi = (nir - red) / (nir + red)
        ndvi = np.nan_to_num(ndvi, nan=0.0, posinf=0.0, neginf=0.0)
    return ndvi

processed_dir = "/content/processed_data"
bands_dir = os.path.join(processed_dir, "bands")
ndvi_dir  = os.path.join(processed_dir, "ndvi")

os.makedirs(bands_dir, exist_ok=True)
os.makedirs(ndvi_dir,  exist_ok=True)

def stack_and_save_5bands(img_id, file_list):
    """
    Reads 5 .tif files => (5, H, W), crops black, NDVI, saves both.
    """
    band_arrays = []
    for tif_name in file_list:
        path = os.path.join(deep_dir_path, tif_name)
        with rasterio.open(path) as src:
            band_arrays.append(src.read(1))  # shape: (H, W)
    band_stack = np.stack(band_arrays, axis=0)  # (5, H, W)

    # Crop
    band_crop = crop_black_borders(band_stack)
    # NDVI
    ndvi_img = calculate_ndvi(band_crop, red_idx=2, nir_idx=4)

    # Save 5-band
    out_bands_path = os.path.join(bands_dir, f"{img_id}_bands.tif")
    with rasterio.open(os.path.join(deep_dir_path, file_list[0])) as ref_src:
        meta = ref_src.meta.copy()
    meta.update({
        'count': 5,
        'dtype': 'float32',
        'height': band_crop.shape[1],
        'width':  band_crop.shape[2]
    })
    with rasterio.open(out_bands_path, 'w', **meta) as dst:
        for i in range(5):
            dst.write(band_crop[i].astype(np.float32), i+1)

    # Save NDVI
    out_ndvi_path = os.path.join(ndvi_dir, f"{img_id}_ndvi.tif")
    meta.update({'count':1})
    with rasterio.open(out_ndvi_path, 'w', **meta) as dst:
        dst.write(ndvi_img.astype(np.float32), 1)

    return band_crop, ndvi_img

# Process all sets
for img_id, files in grouped_files.items():
    stack_and_save_5bands(img_id, files)

print("Processed 5-band & NDVI saved in:", processed_dir)

# PLOT a sample
sample_id = list(grouped_files.keys())[2]
sample_bands_path = os.path.join(bands_dir, f"{sample_id}_bands.tif")
sample_ndvi_path  = os.path.join(ndvi_dir,  f"{sample_id}_ndvi.tif")

with rasterio.open(sample_bands_path) as sb:
    sample_5stack = sb.read()  # shape: (5, H, W)
with rasterio.open(sample_ndvi_path) as sn:
    sample_ndvi = sn.read(1)

plt.figure(figsize=(12,4))
for i in range(5):
    plt.subplot(1,6,i+1)
    plt.imshow(sample_5stack[i], cmap='gray')
    plt.title(f"Band {i+1}")
    plt.axis('off')

plt.subplot(1,6,6)
plt.title("NDVI")
plt.imshow(sample_ndvi, cmap='RdYlGn')
plt.axis('off')
plt.tight_layout()
plt.show()

##############################################
# 5. GENERATE PSEUDO MASK + PLOT
##############################################
masks_dir = os.path.join(processed_dir, "masks")
os.makedirs(masks_dir, exist_ok=True)

def pseudo_panicle_mask(bands_5, ndvi_img,
                        ndvi_min=0.2, ndvi_max=0.6,
                        rededge_idx=3, rededge_thresh=300):
    """
    bands_5 shape: (5, H, W), ndvi_img: (H, W)
    We'll pick NDVI in [ndvi_min, ndvi_max] & RedEdge >= rededge_thresh
    """
    ndvi_mask = (ndvi_img >= ndvi_min) & (ndvi_img <= ndvi_max)
    re_mask   = (bands_5[rededge_idx] >= rededge_thresh)
    combined  = ndvi_mask & re_mask
    mask_bin  = combined.astype(np.uint8)

    # morphological cleaning
    kernel = np.ones((3,3), np.uint8)
    mask_clean = cv2.morphologyEx(mask_bin, cv2.MORPH_OPEN, kernel)
    return mask_clean

import glob

for bp in glob.glob(os.path.join(bands_dir, "*_bands.tif")):
    fname = os.path.basename(bp)
    base_id = fname.replace("_bands.tif","")
    ndvi_path = os.path.join(ndvi_dir,  f"{base_id}_ndvi.tif")
    mask_path = os.path.join(masks_dir, f"{base_id}_mask.tif")

    if os.path.exists(ndvi_path):
        with rasterio.open(bp) as sb:
            bstack = sb.read()  # (5, H, W)
            meta_b = sb.meta.copy()
        with rasterio.open(ndvi_path) as sn:
            ndvi_img = sn.read(1)

        mask_pseudo = pseudo_panicle_mask(bstack, ndvi_img)
        meta_b.update({'count':1, 'dtype':'uint8'})
        with rasterio.open(mask_path, 'w', **meta_b) as dst:
            dst.write(mask_pseudo, 1)

print("Pseudo masks saved in:", masks_dir)

# Plot a sample mask
sample_mask_path = os.path.join(masks_dir, f"{sample_id}_mask.tif")
if os.path.exists(sample_mask_path):
    with rasterio.open(sample_mask_path) as s:
        pseudo_mask = s.read(1)

    plt.figure(figsize=(12,4))
    plt.subplot(1,3,1)
    plt.title("RedEdge (Band 4 in 0-based index 3)")
    plt.imshow(sample_5stack[3], cmap='gray')
    plt.axis('off')

    plt.subplot(1,3,2)
    plt.title("NDVI")
    plt.imshow(sample_ndvi, cmap='RdYlGn')
    plt.axis('off')

    plt.subplot(1,3,3)
    plt.title("Pseudo Mask")
    plt.imshow(pseudo_mask, cmap='gray')
    plt.axis('off')
    plt.tight_layout()
    plt.show()

##############################################
# 6. CREATE DATASET (BANDS+NDVI+MASK)
##############################################
import glob
from sklearn.model_selection import train_test_split

band_files = sorted(glob.glob(os.path.join(bands_dir, "*_bands.tif")))
data_list = []
for bp in band_files:
    base_id = os.path.basename(bp).replace("_bands.tif", "")
    ndvi_p  = os.path.join(ndvi_dir,  f"{base_id}_ndvi.tif")
    mask_p  = os.path.join(masks_dir, f"{base_id}_mask.tif")
    if os.path.exists(ndvi_p) and os.path.exists(mask_p):
        data_list.append((bp, ndvi_p, mask_p))

print("Total processed samples:", len(data_list))

trainval_list, test_list = train_test_split(data_list, test_size=0.2, random_state=42)
train_list, val_list = train_test_split(trainval_list, test_size=0.25, random_state=42)

print(f"Train: {len(train_list)}, Val: {len(val_list)}, Test: {len(test_list)}")

import torch
from torch.utils.data import Dataset, DataLoader

class PaddyPseudoDataset(Dataset):
    """
    Loads (5-band + NDVI) => (6, H, W), plus a mask => (1, H, W).
    Optionally resize for memory saving.
    """
    def __init__(self, samples, resize_hw=(256,256)):
        self.samples = samples
        self.resize_hw = resize_hw

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        band_path, ndvi_path, mask_path = self.samples[idx]

        with rasterio.open(band_path) as sb:
            bands_5 = sb.read().astype(np.float32)  # (5, H, W)
        with rasterio.open(ndvi_path) as sn:
            ndvi_img = sn.read(1).astype(np.float32) # (H, W)
        ndvi_img = np.expand_dims(ndvi_img, axis=0)  # (1, H, W)

        image_6 = np.concatenate([bands_5, ndvi_img], axis=0)  # (6, H, W)

        with rasterio.open(mask_path) as sm:
            mask_img = sm.read(1).astype(np.float32) # (H, W)
        mask_img = np.expand_dims(mask_img, axis=0)  # (1, H, W)

        import cv2
        if self.resize_hw is not None:
            resized_channels = []
            for i in range(6):
                ch = image_6[i]
                ch_res = cv2.resize(ch, (self.resize_hw[1], self.resize_hw[0]),
                                    interpolation=cv2.INTER_AREA)
                resized_channels.append(ch_res)
            image_6 = np.stack(resized_channels, axis=0)

            mk_res = cv2.resize(mask_img[0], (self.resize_hw[1], self.resize_hw[0]),
                                interpolation=cv2.INTER_NEAREST)
            mask_img = np.expand_dims(mk_res, axis=0)

        image_6 = torch.from_numpy(image_6)
        mask_img = torch.from_numpy(mask_img)
        return image_6, mask_img

train_dataset = PaddyPseudoDataset(train_list, resize_hw=(256,256))
val_dataset   = PaddyPseudoDataset(val_list,   resize_hw=(256,256))
test_dataset  = PaddyPseudoDataset(test_list,  resize_hw=(256,256))

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)
val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, num_workers=0)
test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False, num_workers=0)

import torch.nn as nn
import torch.nn.functional as F
import torchvision

def create_resnet_backbone_6ch():
    """
    Creates a ResNet-50 backbone but modifies the first conv to accept 6 input channels.
    We won't load ImageNet pre-trained weights because that is for 3 channels.
    (You could do a partial load, but we keep it simple here.)
    """
    backbone = torchvision.models.resnet50(weights=None)  # no pretrained
    # Modify first conv layer to 6 input channels
    # Original: in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3
    old_conv = backbone.conv1
    new_conv = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)

    # Copy weights from the first 3 channels if you want partial initialization
    # But here we start from scratch.
    # Alternatively: if you want partial loading from pretrained, you'd do that carefully.

    backbone.conv1 = new_conv
    return backbone

class DeepLabHead(nn.Module):
    """
    ASPP + upsampling head for DeepLabv3+.
    """
    def __init__(self, in_channels, num_classes):
        super().__init__()
        self.aspp = torchvision.models.segmentation.deeplabv3.ASPP(
            in_channels=in_channels,
            atrous_rates=(12, 24, 36),
            out_channels=256
        )
        # Additional layers for DeepLabv3+
        self.conv = nn.Sequential(
            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )
        self.classifier = nn.Conv2d(256, num_classes, kernel_size=1)

    def forward(self, x):
        x = self.aspp(x)
        x = self.conv(x)
        x = self.classifier(x)
        return x

class DeepLabV3Plus_6ch(nn.Module):
    """
    Custom DeepLabv3+ model with a ResNet-50 backbone that takes 6 input channels.
    """
    def __init__(self, num_classes=1):
        super().__init__()
        self.backbone = create_resnet_backbone_6ch()
        # Extract the layers
        # Typically, layer4 output used for ASPP
        # We also need the low-level features from earlier layer (DeepLabv3+ uses skip from layer1).
        self.low_level_idx = 1   # in resnet, index: layer1 => (64)
        self.high_level_idx = 4  # in resnet, index: layer4 => (2048)

        # We'll store the blocks in a forward pass for skip connection
        self.aspp_head = DeepLabHead(in_channels=2048, num_classes=256)

        # Low-level reduce
        self.low_level_reduce = nn.Sequential(
            nn.Conv2d(256, 48, kernel_size=1, bias=False),
            nn.BatchNorm2d(48),
            nn.ReLU(inplace=True)
        )
        # Final conv
        self.final_conv = nn.Sequential(
            nn.Conv2d(48+256, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, num_classes, kernel_size=1)
        )

    def _forward_backbone(self, x):
        # For ResNet:
        # x => conv1 => bn1 => relu => maxpool => layer1 => layer2 => layer3 => layer4
        layers = []
        # step through each part
        out = self.backbone.conv1(x)
        out = self.backbone.bn1(out)
        out = self.backbone.relu(out)
        out = self.backbone.maxpool(out)

        # layer1
        out = self.backbone.layer1(out); layers.append(out)  # index 0
        # layer2
        out = self.backbone.layer2(out); layers.append(out)  # index 1
        # layer3
        out = self.backbone.layer3(out); layers.append(out)  # index 2
        # layer4
        out = self.backbone.layer4(out); layers.append(out)  # index 3

        return layers  # [layer1_out, layer2_out, layer3_out, layer4_out]

    def forward(self, x):
        # run backbone
        features = self._forward_backbone(x)
        low_level_feat = features[self.low_level_idx]   # e.g. layer2 output or layer1 output
        high_level_feat = features[self.high_level_idx] # layer4 output => idx=3

        # ASPP on high-level
        aspp_out = self.aspp_head(high_level_feat)  # => shape (N, 256, H/16, W/16)

        # Upsample ASPP output to match low-level
        aspp_out_up = F.interpolate(aspp_out, size=low_level_feat.shape[2:], mode='bilinear', align_corners=False)

        # reduce channels of low-level
        low_level_out = self.low_level_reduce(low_level_feat)  # => (N,48,H/4,W/4) if low_level_idx=1

        # If our high-level is H/16 => up => H/4, we combine
        merged = torch.cat([aspp_out_up, low_level_out], dim=1) # => (N, 48+256, H/4, W/4)

        # final conv
        out = self.final_conv(merged)  # => (N, num_classes, H/4, W/4)

        # final upsample
        out = F.interpolate(out, scale_factor=4, mode='bilinear', align_corners=False)
        return out

import gc

def dice_coeff(pred, target, smooth=1e-5):
    pred_bin = (torch.sigmoid(pred) > 0.5).float()
    intersection = (pred_bin * target).sum(dim=(1,2,3))
    union = pred_bin.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))
    dice = (2.0 * intersection + smooth) / (union + smooth)
    return dice.mean()

def train_one_epoch(model, loader, optimizer, criterion):
    model.train()
    epoch_loss = 0.0
    epoch_dice = 0.0
    for images, masks in loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        epoch_dice += dice_coeff(outputs, masks).item()

        del images, masks, outputs, loss
        gc.collect()

    return epoch_loss / len(loader), epoch_dice / len(loader)

def validate_one_epoch(model, loader, criterion):
    model.eval()
    val_loss = 0.0
    val_dice = 0.0
    with torch.no_grad():
        for images, masks in loader:
            outputs = model(images)
            loss = criterion(outputs, masks)

            val_loss += loss.item()
            val_dice += dice_coeff(outputs, masks).item()

            del images, masks, outputs, loss
            gc.collect()

    return val_loss / len(loader), val_dice / len(loader)

model_deeplab = DeepLabV3Plus_6ch(num_classes=1)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model_deeplab.parameters(), lr=1e-4)

num_epochs = 50  # you can try 40 or more for better performance

train_loss_history = []
train_dice_history = []
val_loss_history   = []
val_dice_history   = []

for epoch in range(num_epochs):
    tr_loss, tr_dice = train_one_epoch(model_deeplab, train_loader, optimizer, criterion)
    va_loss, va_dice = validate_one_epoch(model_deeplab, val_loader, criterion)

    train_loss_history.append(tr_loss)
    train_dice_history.append(tr_dice)
    val_loss_history.append(va_loss)
    val_dice_history.append(va_dice)

    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"| Train Loss: {tr_loss:.4f}, Train Dice: {tr_dice:.4f} "
          f"| Val Loss: {va_loss:.4f}, Val Dice: {va_dice:.4f}")

processed_dir = "/content/processed_data"
bands_dir = os.path.join(processed_dir, "bands")
ndvi_dir  = os.path.join(processed_dir, "ndvi")

os.makedirs(bands_dir, exist_ok=True)
os.makedirs(ndvi_dir,  exist_ok=True)

def stack_and_save_5bands(img_id, file_list):
    """
    1) Read 5 TIF => (5, H, W)
    2) Crop black
    3) NDVI
    4) Save => _bands.tif, _ndvi.tif
    """
    band_arrays = []
    for tif_name in file_list:
        path = os.path.join(deep_dir_path, tif_name)
        with rasterio.open(path) as src:
            band_arrays.append(src.read(1))
    band_stack = np.stack(band_arrays, axis=0)  # (5, H, W)

    cropped = crop_black_borders(band_stack)
    ndvi_img = calculate_ndvi(cropped, red_idx=2, nir_idx=4)

    out_bands_path = os.path.join(bands_dir, f"{img_id}_bands.tif")
    out_ndvi_path  = os.path.join(ndvi_dir,  f"{img_id}_ndvi.tif")

    with rasterio.open(os.path.join(deep_dir_path, file_list[0])) as ref_src:
        meta = ref_src.meta.copy()
    meta.update({
        'count':5,
        'dtype':'float32',
        'height': cropped.shape[1],
        'width':  cropped.shape[2]
    })
    # Save 5-band
    with rasterio.open(out_bands_path, 'w', **meta) as dst:
        for i in range(5):
            dst.write(cropped[i], i+1)

    # Save NDVI
    meta.update({'count':1})
    with rasterio.open(out_ndvi_path, 'w', **meta) as dst:
        dst.write(ndvi_img, 1)

    return cropped, ndvi_img

# Process
for img_id, files in grouped_files.items():
    stack_and_save_5bands(img_id, files)

print("Processed 5-band + NDVI saved in:", processed_dir)

# Plot sample
sample_id = list(grouped_files.keys())[0]
sample_bands_path = os.path.join(bands_dir, f"{sample_id}_bands.tif")
sample_ndvi_path  = os.path.join(ndvi_dir,  f"{sample_id}_ndvi.tif")

with rasterio.open(sample_bands_path) as sb:
    sample_5stack = sb.read()  # shape: (5, H, W)
with rasterio.open(sample_ndvi_path) as sn:
    sample_ndvi = sn.read(1)

plt.figure(figsize=(12,4))
for i in range(5):
    plt.subplot(1,6,i+1)
    plt.imshow(sample_5stack[i], cmap='gray')
    plt.title(f"Band {i+1}")
    plt.axis('off')

plt.subplot(1,6,6)
plt.imshow(sample_ndvi, cmap='RdYlGn')
plt.title("NDVI")
plt.axis('off')
plt.tight_layout()
plt.show()

masks_dir = os.path.join(processed_dir, "masks")
os.makedirs(masks_dir, exist_ok=True)

def pseudo_panicle_mask(bands_5, ndvi_img, ndvi_min=0.2, ndvi_max=0.6,
                        rededge_idx=3, rededge_thresh=300):
    ndvi_mask = (ndvi_img >= ndvi_min) & (ndvi_img <= ndvi_max)
    re_mask   = (bands_5[rededge_idx] >= rededge_thresh)
    combined  = ndvi_mask & re_mask
    mask_bin  = combined.astype(np.uint8)

    kernel = np.ones((3,3), np.uint8)
    mask_clean = cv2.morphologyEx(mask_bin, cv2.MORPH_OPEN, kernel)
    return mask_clean

import glob

for bp in glob.glob(os.path.join(bands_dir, "*_bands.tif")):
    base_id = os.path.basename(bp).replace("_bands.tif","")
    ndvi_path = os.path.join(ndvi_dir,  f"{base_id}_ndvi.tif")
    mask_path = os.path.join(masks_dir, f"{base_id}_mask.tif")
    if os.path.exists(ndvi_path):
        with rasterio.open(bp) as sb:
            bstack = sb.read()
            meta_b = sb.meta.copy()
        with rasterio.open(ndvi_path) as sn:
            ndvi_img = sn.read(1)

        pmask = pseudo_panicle_mask(bstack, ndvi_img)
        meta_b.update({'count':1, 'dtype':'uint8'})
        with rasterio.open(mask_path, 'w', **meta_b) as dst:
            dst.write(pmask, 1)

print("Pseudo masks in:", masks_dir)

# Plot sample
sample_mask_path = os.path.join(masks_dir, f"{sample_id}_mask.tif")
if os.path.exists(sample_mask_path):
    with rasterio.open(sample_mask_path) as s:
        sample_mask = s.read(1)

    plt.figure(figsize=(12,4))
    plt.subplot(1,3,1)
    plt.imshow(sample_5stack[3], cmap='gray')
    plt.title("RedEdge band")
    plt.axis('off')

    plt.subplot(1,3,2)
    plt.imshow(sample_ndvi, cmap='RdYlGn')
    plt.title("NDVI")
    plt.axis('off')

    plt.subplot(1,3,3)
    plt.imshow(sample_mask, cmap='gray')
    plt.title("Pseudo Mask")
    plt.axis('off')
    plt.tight_layout()
    plt.show()

import glob
from sklearn.model_selection import train_test_split

band_files = sorted(glob.glob(os.path.join(bands_dir, "*_bands.tif")))
data_list = []
for bp in band_files:
    base_id = os.path.basename(bp).replace("_bands.tif","")
    ndvi_p = os.path.join(ndvi_dir,  f"{base_id}_ndvi.tif")
    mk_p   = os.path.join(masks_dir, f"{base_id}_mask.tif")
    if os.path.exists(ndvi_p) and os.path.exists(mk_p):
        data_list.append((bp, ndvi_p, mk_p))

print("Total samples:", len(data_list))

trainval_list, test_list = train_test_split(data_list, test_size=0.2, random_state=42)
train_list, val_list     = train_test_split(trainval_list, test_size=0.25, random_state=42)
print(f"Train: {len(train_list)}, Val: {len(val_list)}, Test: {len(test_list)}")

import torch
from torch.utils.data import Dataset, DataLoader
import cv2

class PaddyPseudoDataset(Dataset):
    def __init__(self, samples, resize_hw=(256,256)):
        self.samples = samples
        self.resize_hw = resize_hw

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        b_path, n_path, m_path = self.samples[idx]

        with rasterio.open(b_path) as sb:
            bands_5 = sb.read().astype(np.float32)  # (5, H, W)
        with rasterio.open(n_path) as sn:
            ndvi = sn.read(1).astype(np.float32)    # (H, W)
        ndvi = np.expand_dims(ndvi, axis=0)

        image_6 = np.concatenate([bands_5, ndvi], axis=0)  # (6, H, W)

        with rasterio.open(m_path) as sm:
            mask_img = sm.read(1).astype(np.float32)        # (H, W)
        mask_img = np.expand_dims(mask_img, axis=0)         # (1, H, W)

        if self.resize_hw is not None:
            resized_channels = []
            for i in range(6):
                ch = image_6[i]
                ch_res = cv2.resize(ch, (self.resize_hw[1], self.resize_hw[0]), interpolation=cv2.INTER_AREA)
                resized_channels.append(ch_res)
            image_6 = np.stack(resized_channels, axis=0)

            mk_res = cv2.resize(mask_img[0], (self.resize_hw[1], self.resize_hw[0]),
                                interpolation=cv2.INTER_NEAREST)
            mask_img = np.expand_dims(mk_res, axis=0)

        image_6 = torch.from_numpy(image_6)
        mask_img = torch.from_numpy(mask_img)
        return image_6, mask_img

train_dataset = PaddyPseudoDataset(train_list, (256,256))
val_dataset   = PaddyPseudoDataset(val_list,   (256,256))
test_dataset  = PaddyPseudoDataset(test_list,  (256,256))

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)
val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, num_workers=0)
test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False, num_workers=0)

import torch.nn as nn
import torch.nn.functional as F
import torchvision

def create_resnet_backbone_6ch():
    backbone = torchvision.models.resnet50(weights=None)  # no pretrained
    # Modify conv1 to 6 channels
    old_conv = backbone.conv1
    new_conv = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)
    # (If you wanted partial weight copying from pretrained 3 channels, you'd do that carefully here.)
    backbone.conv1 = new_conv
    return backbone

class DeepLabHead(nn.Module):
    """
    The ASPP + conv classifier head for DeepLabv3+.
    """
    def __init__(self, in_channels, num_classes):
        super().__init__()
        self.aspp = torchvision.models.segmentation.deeplabv3.ASPP(
            in_channels=in_channels,
            atrous_rates=(12, 24, 36),
            out_channels=256
        )
        self.conv = nn.Sequential(
            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )
        self.classifier = nn.Conv2d(256, num_classes, kernel_size=1)

    def forward(self, x):
        x = self.aspp(x)
        x = self.conv(x)
        x = self.classifier(x)
        return x

class DeepLabV3Plus_6ch(nn.Module):
    """
    Custom DeepLabv3+ with ResNet-50 backbone for 6-channel input.
    """
    def __init__(self, num_classes=1):
        super().__init__()
        self.backbone = create_resnet_backbone_6ch()

        # We'll store layers [layer1, layer2, layer3, layer4] => 4 outputs
        # Indices: 0=layer1, 1=layer2, 2=layer3, 3=layer4
        self.low_level_idx  = 0  # use layer1 as low-level skip
        self.high_level_idx = 3  # layer4 is the final

        self.aspp_head = DeepLabHead(in_channels=2048, num_classes=256)

        # low-level output from layer1 => channels=256 in ResNet-50
        self.low_level_reduce = nn.Sequential(
            nn.Conv2d(256, 48, kernel_size=1, bias=False),
            nn.BatchNorm2d(48),
            nn.ReLU(inplace=True)
        )

        # final conv after merging
        self.final_conv = nn.Sequential(
            nn.Conv2d(48 + 256, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, num_classes, kernel_size=1)
        )

    def _forward_backbone(self, x):
        """
        We'll manually run: conv1->bn1->relu->maxpool->layer1->layer2->layer3->layer4
        and store them in a list => [layer1_out, layer2_out, layer3_out, layer4_out].
        """
        layers = []

        out = self.backbone.conv1(x)
        out = self.backbone.bn1(out)
        out = self.backbone.relu(out)
        out = self.backbone.maxpool(out)

        # layer1
        out = self.backbone.layer1(out)
        layers.append(out)   # index=0

        # layer2
        out = self.backbone.layer2(out)
        layers.append(out)   # index=1

        # layer3
        out = self.backbone.layer3(out)
        layers.append(out)   # index=2

        # layer4
        out = self.backbone.layer4(out)
        layers.append(out)   # index=3

        return layers

    def forward(self, x):
        features = self._forward_backbone(x)
        low_level_feat  = features[self.low_level_idx]   # layer1
        high_level_feat = features[self.high_level_idx]  # layer4

        # ASPP on high-level => shape => (N, 256, H/16, W/16)
        aspp_out = self.aspp_head(high_level_feat)

        # upsample ASPP to match low-level spatial dims
        aspp_out_up = F.interpolate(aspp_out, size=low_level_feat.shape[2:],
                                    mode='bilinear', align_corners=False)

        # reduce low-level from 256->48 channels
        low_level_out = self.low_level_reduce(low_level_feat)

        # merge
        merged = torch.cat([aspp_out_up, low_level_out], dim=1)  # => (N, 48+256, H/4, W/4) if layer1 => H/4?

        # final conv
        out = self.final_conv(merged)  # => (N, num_classes, H/4, W/4)

        # final upsample to H, W
        out = F.interpolate(out, scale_factor=4, mode='bilinear', align_corners=False)
        return out

import gc
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

def dice_coeff(pred, target, smooth=1e-5):
    pred_bin = (torch.sigmoid(pred) > 0.5).float()
    intersection = (pred_bin * target).sum(dim=(1,2,3))
    union = pred_bin.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))
    dice = (2.0 * intersection + smooth) / (union + smooth)
    return dice.mean()

def train_one_epoch(model, loader, optimizer, criterion):
    model.train()
    epoch_loss = 0.0
    epoch_dice = 0.0

    for images, masks in loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        epoch_dice += dice_coeff(outputs, masks).item()

        del images, masks, outputs, loss
        gc.collect()

    return epoch_loss / len(loader), epoch_dice / len(loader)

def validate_one_epoch(model, loader, criterion):
    model.eval()
    val_loss = 0.0
    val_dice = 0.0
    with torch.no_grad():
        for images, masks in loader:
            outputs = model(images)
            loss = criterion(outputs, masks)
            val_loss += loss.item()
            val_dice += dice_coeff(outputs, masks).item()

            del images, masks, outputs, loss
            gc.collect()

    return val_loss / len(loader), val_dice / len(loader)

model_deeplab = DeepLabV3Plus_6ch(num_classes=1)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model_deeplab.parameters(), lr=1e-4)

num_epochs = 50  # Increase to 40 or more if you have time/data
train_loss_history = []
train_dice_history = []
val_loss_history   = []
val_dice_history   = []

for epoch in range(num_epochs):
    tr_loss, tr_dice = train_one_epoch(model_deeplab, train_loader, optimizer, criterion)
    va_loss, va_dice = validate_one_epoch(model_deeplab, val_loader, criterion)

    train_loss_history.append(tr_loss)
    train_dice_history.append(tr_dice)
    val_loss_history.append(va_loss)
    val_dice_history.append(va_dice)

    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"| Train Loss: {tr_loss:.4f}, Train Dice: {tr_dice:.4f} "
          f"| Val Loss: {va_loss:.4f}, Val Dice: {va_dice:.4f}")

file_path = '/content/data_unzipped/Agri/Paddy/paddy_season4_RededgeMultispectral_20190917_05m/paramlog.dat'

# Try to open the file as text
try:
    with open(file_path, 'r') as file:
        content = file.read(500)  # Read first 500 characters
        print("Text content:\n", content)
except UnicodeDecodeError:
    print("The file is binary.")

import numpy as np

# Load the binary data (assuming it's float32, adjust if necessary)
data = np.fromfile(file_path, dtype=np.float32)  # Change dtype if needed
print("Data shape:", data.shape)
print("First 10 values:", data[:10])



import matplotlib.pyplot as plt

# Example: 128x128 (adjust as needed)
try:
    mask = data_int.reshape(-1, 128, 128)  # For multiple masks
    plt.imshow(mask[0], cmap='gray')
    plt.title("Possible Mask")
    plt.axis('off')
    plt.show()
except:
    print("Reshaping failed. Likely not a mask.")

with open('/content/data_unzipped/Agri/Paddy/paddy_season4_RededgeMultispectral_20190917_05m/diag.dat', 'r') as file:
    print(file.read(500))

data_int = np.fromfile(file_path, dtype=np.uint8)  # Try uint8 for masks
print("Data shape:", data_int.shape)
print("First 20 values:", data_int[:20])

import os

# Path to the folder containing the .tif images
image_folder = '/content/data_unzipped/Agri/Paddy/paddy_season4_RededgeMultispectral_20190917_05m/000'

# List all .tif files
tif_files = [f for f in os.listdir(image_folder) if f.endswith('.tif')]
print("Total .tif files:", len(tif_files))
print("Sample files:", tif_files[:10])

import rasterio

for file in tif_files:
    file_path = os.path.join(image_folder, file)

    with rasterio.open(file_path) as img:
        data = img.read(1)  # Read the first band
        unique_values = set(data.flatten())

        print(f"File: {file}")
        print(f"Shape: {data.shape}")
        print(f"Unique values: {list(unique_values)[:10]}")
        print("-" * 40)

from collections import defaultdict

# Group files by the base filename
grouped_files = defaultdict(list)

for file in tif_files:
    base_name = '_'.join(file.split('_')[:-1])  # Adjust split logic if needed
    grouped_files[base_name].append(file)

for key, files in grouped_files.items():
    print(f"Base Image: {key}")
    print(f"Bands: {files}")
    print("-" * 40)

import matplotlib.pyplot as plt

# Replace 'suspected_mask.tif' with a file you suspect is a mask
file_path = os.path.join(image_folder, 'suspected_mask.tif')

with rasterio.open(file_path) as img:
    mask_data = img.read(1)  # Read first band

plt.imshow(mask_data, cmap='gray')
plt.title("Suspected Mask Visualization")
plt.axis('off')
plt.show()

import os

# Path to the folder containing the .tif images
image_folder = '/content/data_unzipped/Agri/Paddy/paddy_season4_RededgeMultispectral_20190917_05m/000'

# List all .tif files
tif_files = [f for f in os.listdir(image_folder) if f.endswith('.tif')]
print("Total .tif files:", len(tif_files))
print("Sample files:", tif_files[:10])

import rasterio

# Check metadata for the first few files
for file in tif_files[:5]:
    file_path = os.path.join(image_folder, file)

    with rasterio.open(file_path) as img:
        print(f"Metadata for {file}:")
        print(img.meta)
        print("-" * 50)

for file in tif_files:
    file_path = os.path.join(image_folder, file)

    with rasterio.open(file_path) as img:
        data = img.read(1)  # Read the first band
        unique_values = set(data.flatten())

        print(f"File: {file}")
        print(f"Unique values: {sorted(list(unique_values))[:10]}")
        print("-" * 40)

masks_dir = os.path.join(processed_dir, "masks")
os.makedirs(masks_dir, exist_ok=True)

def pseudo_panicle_mask(bands_5, ndvi_img,
                        ndvi_min=0.2, ndvi_max=0.6,
                        rededge_idx=3, rededge_thresh=300):
    ndvi_mask = (ndvi_img >= ndvi_min) & (ndvi_img <= ndvi_max)
    re_mask   = (bands_5[rededge_idx] >= rededge_thresh)
    combined  = ndvi_mask & re_mask
    mask_bin  = combined.astype(np.uint8)

    kernel = np.ones((3,3), np.uint8)
    mask_clean = cv2.morphologyEx(mask_bin, cv2.MORPH_OPEN, kernel)
    return mask_clean

import glob

for bp in glob.glob(os.path.join(bands_dir, "*_bands.tif")):
    base_id = os.path.basename(bp).replace("_bands.tif","")
    ndvi_p  = os.path.join(ndvi_dir, f"{base_id}_ndvi.tif")
    mask_p  = os.path.join(masks_dir, f"{base_id}_mask.tif")

    if os.path.exists(ndvi_p):
        with rasterio.open(bp) as sb:
            bstack = sb.read()
            meta_b = sb.meta.copy()
        with rasterio.open(ndvi_p) as sn:
            ndvi_img = sn.read(1)

        pmask = pseudo_panicle_mask(bstack, ndvi_img)
        meta_b.update({'count':1, 'dtype':'uint8'})
        with rasterio.open(mask_p, 'w', **meta_b) as dst:
            dst.write(pmask, 1)

print("Pseudo masks in:", masks_dir)

# Plot sample
sample_mask_path = os.path.join(masks_dir, f"{sample_id}_mask.tif")
if os.path.exists(sample_mask_path):
    with rasterio.open(sample_mask_path) as s:
        sample_mask = s.read(1)

    plt.figure(figsize=(12,4))
    plt.subplot(1,3,1)
    plt.title("RedEdge band")
    plt.imshow(sample_5stack[3], cmap='gray')
    plt.axis('off')

    plt.subplot(1,3,2)
    plt.title("NDVI")
    plt.imshow(sample_ndvi, cmap='RdYlGn')
    plt.axis('off')

    plt.subplot(1,3,3)
    plt.title("Pseudo Mask")
    plt.imshow(sample_mask, cmap='gray')
    plt.axis('off')
    plt.tight_layout()
    plt.show()

band_files = sorted(glob.glob(os.path.join(bands_dir, "*_bands.tif")))
data_list = []
for bp in band_files:
    base_id = os.path.basename(bp).replace("_bands.tif","")
    ndvi_p  = os.path.join(ndvi_dir,  f"{base_id}_ndvi.tif")
    mk_p    = os.path.join(masks_dir, f"{base_id}_mask.tif")
    if os.path.exists(ndvi_p) and os.path.exists(mk_p):
        data_list.append((bp, ndvi_p, mk_p))

print("Data samples:", len(data_list))

from sklearn.model_selection import train_test_split

trainval_list, test_list = train_test_split(data_list, test_size=0.2, random_state=42)
train_list, val_list     = train_test_split(trainval_list, test_size=0.25, random_state=42)
print(f"Train: {len(train_list)}, Val: {len(val_list)}, Test: {len(test_list)}")

import torch
from torch.utils.data import Dataset, DataLoader
import cv2

class PaddyPseudoDataset(Dataset):
    def __init__(self, samples, resize_hw=(256,256)):
        self.samples = samples
        self.resize_hw = resize_hw

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        b_path, n_path, m_path = self.samples[idx]

        with rasterio.open(b_path) as sb:
            bands_5 = sb.read().astype(np.float32)  # (5, H, W)
        with rasterio.open(n_path) as sn:
            ndvi_img = sn.read(1).astype(np.float32) # (H, W)
        ndvi_img = np.expand_dims(ndvi_img, axis=0)  # => (1,H,W)

        image_6 = np.concatenate([bands_5, ndvi_img], axis=0)  # => (6,H,W)

        with rasterio.open(m_path) as sm:
            mask_img = sm.read(1).astype(np.float32)  # => (H,W)
        mask_img = np.expand_dims(mask_img, axis=0)  # => (1,H,W)

        if self.resize_hw is not None:
            resized_chans = []
            for i in range(6):
                ch = image_6[i]
                ch_res = cv2.resize(ch, (self.resize_hw[1], self.resize_hw[0]), interpolation=cv2.INTER_AREA)
                resized_chans.append(ch_res)
            image_6 = np.stack(resized_chans, axis=0)

            mk_res = cv2.resize(mask_img[0], (self.resize_hw[1], self.resize_hw[0]),
                                interpolation=cv2.INTER_NEAREST)
            mask_img = np.expand_dims(mk_res, axis=0)

        image_6 = torch.from_numpy(image_6)
        mask_img = torch.from_numpy(mask_img)
        return image_6, mask_img

train_dataset = PaddyPseudoDataset(train_list, (256,256))
val_dataset   = PaddyPseudoDataset(val_list,   (256,256))
test_dataset  = PaddyPseudoDataset(test_list,  (256,256))

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)
val_loader   = DataLoader(val_dataset,   batch_size=8, shuffle=False, num_workers=0)
test_loader  = DataLoader(test_dataset,  batch_size=16, shuffle=False, num_workers=0)

import torch
import torch.nn as nn
import torch.nn.functional as F

# -- Basic building blocks: Patch Embedding, Swin blocks, etc.

class PatchEmbed(nn.Module):
    """
    Embeds the input images by splitting them into patches and applying a linear embedding.
    We adapt it to handle 6 channels instead of 3.
    """
    def __init__(self, patch_size=4, in_chans=6, embed_dim=96):
        super().__init__()
        self.patch_size = patch_size
        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim,
                              kernel_size=patch_size,
                              stride=patch_size)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        # x: (B, 6, H, W)
        B, C, H, W = x.shape
        # reduce size => (B, embed_dim, H/patch, W/patch)
        x = self.proj(x)
        # flatten => (B, embed_dim, num_patches)
        x = x.flatten(2).transpose(1, 2)  # => (B, nPatches, embed_dim)
        x = self.norm(x)
        return x

# We would define SwinTransformerBlock, SwinStage, etc.
# For brevity, we keep it short. We'll assume we have a "SwinEncoder" that returns
# multiple feature maps for U-Net style decoder.

class DummySwinEncoder(nn.Module):
    """
    A placeholder 'Swin' encoder that returns feature maps at 4 levels.
    (In a real scenario, you'd implement the Swin blocks or import from a library.)
    """
    def __init__(self, in_chans=6, embed_dim=96):
        super().__init__()
        self.patch_embed = PatchEmbed(patch_size=4, in_chans=in_chans, embed_dim=embed_dim)

        # We'll just simulate 4 stages with linear layers.
        self.stage1 = nn.Linear(embed_dim, embed_dim)
        self.stage2 = nn.Linear(embed_dim, embed_dim*2)
        self.stage3 = nn.Linear(embed_dim*2, embed_dim*4)
        self.stage4 = nn.Linear(embed_dim*4, embed_dim*8)

        # The real Swin would have windowed MSA, shifting, etc.

    def forward(self, x):
        """
        x: (B,6,H,W)
        We'll produce 4 feature maps (like a U-Net encoder)
        but in practice these are 1D embeddings we might reshape back.
        For demonstration, we'll just show dimension transformations.
        """
        # patch embed
        x = self.patch_embed(x)   # => (B, nPatches, embed_dim)
        B, N, C = x.shape

        # Stage1
        x1 = self.stage1(x)  # => (B,N,embed_dim)
        # Stage2
        x2 = self.stage2(x1) # => (B,N,2*embed_dim)
        # Stage3
        x3 = self.stage3(x2) # => (B,N,4*embed_dim)
        # Stage4
        x4 = self.stage4(x3) # => (B,N,8*embed_dim)

        # For "U-Net" style, we'd reshape each stage into (B,Channels,H',W')
        # We'll just store them as flattened for demonstration.
        return x1, x2, x3, x4

# The decoder would upsample these 4 features back to the original size

class DummySwinUNetDecoder(nn.Module):
    """
    Very simplistic decoder that fakes some upsampling steps.
    In a real approach, you'd reshape the tokens back to spatial,
    then do skip connections, upsample, etc.
    """
    def __init__(self, embed_dim=96, out_channels=1):
        super().__init__()
        # We'll define a few linear layers to simulate "decoding".
        # A real approach would have pixel shuffle or token rearrangements.
        self.linear4 = nn.Linear(embed_dim*8, embed_dim*4)
        self.linear3 = nn.Linear(embed_dim*4, embed_dim*2)
        self.linear2 = nn.Linear(embed_dim*2, embed_dim)
        self.linear1 = nn.Linear(embed_dim, out_channels)

    def forward(self, feats):
        # feats = (x1, x2, x3, x4)
        x1, x2, x3, x4 = feats
        # Start from x4
        B, N, C = x4.shape
        d4 = self.linear4(x4)  # => (B,N,4*embed_dim)
        d3 = self.linear3(d4)  # => (B,N,2*embed_dim)
        d2 = self.linear2(d3)  # => (B,N,embed_dim)
        d1 = self.linear1(d2)  # => (B,N,out_channels)

        # Suppose we reshape back to 2D (dummy approach)
        # We know N= (H/4) * (W/4) if patch_size=4, roughly
        # We'll just do a rough guess. This is very simplified.

        # Output shape => (B, out_channels, H, W)
        # Real Swin-UNet code is more involved. This is purely illustrative.

        return d1

class SwinUNet6ch(nn.Module):
    """
    A highly simplified "Swin-UNet" style model adapted for 6 input channels.
    In practice, you'd use a real implementation with token rearrangements, skip merges, etc.
    """
    def __init__(self, in_chans=6, out_ch=1, embed_dim=96):
        super().__init__()
        self.encoder = DummySwinEncoder(in_chans=in_chans, embed_dim=embed_dim)
        self.decoder = DummySwinUNetDecoder(embed_dim=embed_dim, out_channels=out_ch)

    def forward(self, x):
        # Encode => 4 feature maps
        feats = self.encoder(x)  # (x1,x2,x3,x4)
        # Decode => final output
        out = self.decoder(feats)  # shape => (B,N,out_ch)
        # We'll do a quick reshape guess to (B,out_ch,H,W).
        B, N, C = out.shape
        # Suppose our input was (B,6,H,W), patch_size=4 => N ~ (H/4 * W/4).
        # We can't do an exact reshape without storing H/W.
        # We'll store them from x in the forward call. Let's do that.

        return out  # (B,N, out_ch) => a 2D restructure is needed for real usage

class PatchEmbed(nn.Module):
    def __init__(self, patch_size=4, in_chans=6, embed_dim=96):
        super().__init__()
        self.patch_size = patch_size
        self.in_chans   = in_chans
        self.embed_dim  = embed_dim
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.norm = nn.LayerNorm(embed_dim)
        self.H = None
        self.W = None

    def forward(self, x):
        B, C, H, W = x.shape
        self.H, self.W = H, W
        # # patch embed
        x = self.proj(x)   # => (B,embed_dim,H/patch,W/patch)
        _, E, Hp, Wp = x.shape
        x = x.flatten(2).transpose(1,2)  # => (B,Hp*Wp, E)
        x = self.norm(x)
        return x, (Hp, Wp)

# Then update DummySwinEncoder and DummySwinUNetDecoder to handle shapes for a real "Swin-UNet."

# For brevity, let's keep the concept. We won't rewrite the entire architecture in detail,
# but assume we can produce a (B, out_ch, H, W) final output.

import gc

def dice_coeff(pred, target, smooth=1e-5):
    pred_bin = (torch.sigmoid(pred) > 0.5).float()
    intersection = (pred_bin * target).sum(dim=(1,2,3))
    union = pred_bin.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))
    dice = (2.0 * intersection + smooth) / (union + smooth)
    return dice.mean()

def reshape_output_simplified(out_flat, patch_size=4, H=256, W=256):
    """
    out_flat: shape (B, N, 1)
    We'll guess N = (H/patch_size)*(W/patch_size).
    We'll expand up to (B,1,H,W) by naive upsampling x4 to get the final shape.
    """
    B, N, C = out_flat.shape
    # Reshape to (B,1,H//4, W//4)
    H_small, W_small = H//patch_size, W//patch_size
    out2d = out_flat.transpose(1,2).reshape(B, C, H_small, W_small)
    # Upsample back
    out_up = F.interpolate(out2d, scale_factor=patch_size, mode='bilinear', align_corners=False)
    return out_up

import torch.nn as nn
import torch.optim as optim

model_swin = SwinUNet6ch(in_chans=6, out_ch=1, embed_dim=96)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model_swin.parameters(), lr=1e-4)

num_epochs = 30
train_loss_history = []
train_dice_history = []
val_loss_history   = []
val_dice_history   = []

def train_one_epoch_swin(model, loader, optimizer, criterion):
    model.train()
    epoch_loss = 0.0
    epoch_dice = 0.0

    for images, masks in loader:
        optimizer.zero_grad()
        out_flat = model(images)  # => shape (B, N, out_ch)
        # reshape
        pred = reshape_output_simplified(out_flat, patch_size=4, H=images.shape[2], W=images.shape[3])
        loss = criterion(pred, masks)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        epoch_dice += dice_coeff(pred, masks).item()

        del images, masks, out_flat, pred, loss
        gc.collect()

    return epoch_loss / len(loader), epoch_dice / len(loader)

def validate_one_epoch_swin(model, loader, criterion):
    model.eval()
    val_loss = 0.0
    val_dice = 0.0
    with torch.no_grad():
        for images, masks in loader:
            out_flat = model(images)
            pred = reshape_output_simplified(out_flat, 4, images.shape[2], images.shape[3])
            loss = criterion(pred, masks)

            val_loss += loss.item()
            val_dice += dice_coeff(pred, masks).item()

            del images, masks, out_flat, pred, loss
            gc.collect()

    return val_loss / len(loader), val_dice / len(loader)

for epoch in range(num_epochs):
    tr_loss, tr_dice = train_one_epoch_swin(model_swin, train_loader, optimizer, criterion)
    va_loss, va_dice = validate_one_epoch_swin(model_swin, val_loader, criterion)

    train_loss_history.append(tr_loss)
    train_dice_history.append(tr_dice)
    val_loss_history.append(va_loss)
    val_dice_history.append(va_dice)

    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"| Train Loss: {tr_loss:.4f}, Train Dice: {tr_dice:.4f} "
          f"| Val Loss: {va_loss:.4f}, Val Dice: {va_dice:.4f}")

import torch
import torch.nn as nn
import torch.nn.functional as F

class PatchEmbed(nn.Module):
    """
    Embeds the input by splitting into patches (patch_size x patch_size),
    for in_chans=6, then projects to 'embed_dim' channels.

    Returns:
      x: (B, num_patches, embed_dim)
      (Hp, Wp): the patch-grid size
    """
    def __init__(self, patch_size=4, in_chans=6, embed_dim=96):
        super().__init__()
        self.patch_size = patch_size
        self.in_chans   = in_chans
        self.embed_dim  = embed_dim

        # Convolution to get embeddings
        self.proj = nn.Conv2d(in_chans, embed_dim,
                              kernel_size=patch_size,
                              stride=patch_size)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        # x: (B,6,H,W)
        B, C, H, W = x.shape
        # convolving => shape: (B,embed_dim,H//patch_size, W//patch_size)
        x = self.proj(x)
        # flatten => (B, embed_dim, num_patches) => (B, num_patches, embed_dim)
        B, E, Hp, Wp = x.shape
        x = x.flatten(2).transpose(1,2)  # => (B, Hp*Wp, E)
        x = self.norm(x)
        return x, (Hp, Wp)

class DummySwinEncoder(nn.Module):
    """
    A placeholder "Swin" encoder that returns four feature maps in embeddings form.
    In reality, Swin Transformer blocks do more complicated steps.
    """
    def __init__(self, in_chans=6, embed_dim=96):
        super().__init__()
        self.patch_embed = PatchEmbed(patch_size=4, in_chans=in_chans, embed_dim=embed_dim)

        # We'll define four linear transformations to simulate 4 stages
        self.stage1 = nn.Linear(embed_dim, embed_dim)       # (B,N, embed_dim)
        self.stage2 = nn.Linear(embed_dim, embed_dim*2)     # ...
        self.stage3 = nn.Linear(embed_dim*2, embed_dim*4)
        self.stage4 = nn.Linear(embed_dim*4, embed_dim*8)

    def forward(self, x):
        # patch embedding => x_emb, (Hp, Wp)
        x_emb, (Hp, Wp) = self.patch_embed(x)  # x_emb => (B, N, embed_dim)

        # Stage1
        x1 = self.stage1(x_emb)  # => (B,N, embed_dim)
        # Stage2
        x2 = self.stage2(x1)     # => (B,N, embed_dim*2)
        # Stage3
        x3 = self.stage3(x2)     # => (B,N, embed_dim*4)
        # Stage4
        x4 = self.stage4(x3)     # => (B,N, embed_dim*8)

        return (x1, x2, x3, x4), (Hp, Wp)
        # We'll also return (Hp, Wp) so the decoder can reshape accordingly

class DummySwinUNetDecoder(nn.Module):
    """
    Minimal: We'll just take x4 as the main feature, do some linear transforms,
    then upsample to (H, W).
    """
    def __init__(self, embed_dim=96, out_channels=1):
        super().__init__()
        # Let's say x4 has embed_dim*8 channels
        self.linear_4toFinal = nn.Linear(embed_dim*8, out_channels)  # final channel dimension

    def forward(self, feats, patch_hw, patch_size=4):
        """
        feats: (x1, x2, x3, x4) each => (B,N,Channels)
        patch_hw: (Hp, Wp)
        """
        x1, x2, x3, x4 = feats
        Hp, Wp = patch_hw  # the patch-grid dimension

        # We'll just use x4 for demonstration
        B, N, C = x4.shape
        # linear reduce from embed_dim*8 => out_channels
        out_flat = self.linear_4toFinal(x4)  # => (B, N, out_channels)

        # now reshape to (B, out_channels, Hp, Wp)
        out_flat = out_flat.permute(0,2,1)  # => (B, out_channels, N)
        out_2d = out_flat.reshape(B, -1, Hp, Wp)

        # final upsample by patch_size => (B, out_channels, patch_size*Hp, patch_size*Wp)
        out_img = F.interpolate(out_2d, scale_factor=patch_size, mode='bilinear', align_corners=False)

        return out_img

class SwinUNet6ch(nn.Module):
    """
    Minimal Swin-UNet for 6 channels.
    """
    def __init__(self, in_chans=6, out_ch=1, embed_dim=96):
        super().__init__()
        self.encoder = DummySwinEncoder(in_chans=in_chans, embed_dim=embed_dim)
        self.decoder = DummySwinUNetDecoder(embed_dim=embed_dim, out_channels=out_ch)

    def forward(self, x):
        # encode => feats, (Hp, Wp)
        feats, (Hp, Wp) = self.encoder(x)  # feats => (x1,x2,x3,x4)
        # decode => (B, out_ch, H, W)
        out = self.decoder(feats, (Hp, Wp), patch_size=4)
        return out

import gc
import torch.nn as nn
import torch.optim as optim

def dice_coeff(pred, target, smooth=1e-5):
    pred_bin = (torch.sigmoid(pred) > 0.5).float()
    intersection = (pred_bin * target).sum(dim=(1,2,3))
    union = pred_bin.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))
    dice = (2.0 * intersection + smooth) / (union + smooth)
    return dice.mean()

def train_one_epoch_swin(model, loader, optimizer, criterion):
    model.train()
    epoch_loss = 0.0
    epoch_dice = 0.0
    for images, masks in loader:
        optimizer.zero_grad()
        outputs = model(images)  # => (B, out_ch, H, W)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        epoch_dice += dice_coeff(outputs, masks).item()

        del images, masks, outputs, loss
        gc.collect()

    return epoch_loss / len(loader), epoch_dice / len(loader)

def validate_one_epoch_swin(model, loader, criterion):
    model.eval()
    val_loss = 0.0
    val_dice = 0.0
    with torch.no_grad():
        for images, masks in loader:
            outputs = model(images)
            loss = criterion(outputs, masks)

            val_loss += loss.item()
            val_dice += dice_coeff(outputs, masks).item()

            del images, masks, outputs, loss
            gc.collect()
    return val_loss / len(loader), val_dice / len(loader)

# Instantiate
model_swin = SwinUNet6ch(in_chans=6, out_ch=1, embed_dim=96)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model_swin.parameters(), lr=1e-4)
num_epochs = 100

train_loss_history = []
train_dice_history = []
val_loss_history   = []
val_dice_history   = []

for epoch in range(num_epochs):
    tr_loss, tr_dice = train_one_epoch_swin(model_swin, train_loader, optimizer, criterion)
    va_loss, va_dice = validate_one_epoch_swin(model_swin, val_loader, criterion)

    train_loss_history.append(tr_loss)
    train_dice_history.append(tr_dice)
    val_loss_history.append(va_loss)
    val_dice_history.append(va_dice)

    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"| Train Loss: {tr_loss:.4f}, Train Dice: {tr_dice:.4f} "
          f"| Val Loss: {va_loss:.4f}, Val Dice: {va_dice:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(train_loss_history, label='Train Loss')
plt.plot(val_loss_history,   label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Swin-UNet - Loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(train_dice_history, label='Train Dice')
plt.plot(val_dice_history,   label='Val Dice')
plt.xlabel('Epoch')
plt.ylabel('Dice')
plt.title('Swin-UNet - Dice')
plt.legend()

plt.show()

test_loss = 0.0
test_dice = 0.0
model_swin.eval()
with torch.no_grad():
    for images, masks in test_loader:
        outputs = model_swin(images)
        loss = criterion(outputs, masks)
        test_loss += loss.item()
        test_dice += dice_coeff(outputs, masks).item()

        del images, masks, outputs, loss
        gc.collect()

test_loss /= len(test_loader)
test_dice /= len(test_loader)
print(f"Test Loss: {test_loss:.4f}, Test Dice: {test_dice:.4f}")

import random

n_samples = 3
indices = random.sample(range(len(test_dataset)), n_samples)

plt.figure(figsize=(12, 4*n_samples))
for i, idx in enumerate(indices):
    image_6, mask_gt = test_dataset[idx]  # shape: (6,H,W), (1,H,W)
    image_6_batch = image_6.unsqueeze(0)

    with torch.no_grad():
        pred = model_swin(image_6_batch)  # => (1,1,H,W)
    pred_sigmoid = torch.sigmoid(pred)
    pred_bin = (pred_sigmoid > 0.5).float()

    pred_np = pred_bin.squeeze().numpy()
    mask_np = mask_gt.squeeze().numpy()
    ndvi_np = image_6[5].numpy()  # channel 5 => NDVI

    plt.subplot(n_samples, 3, i*3 + 1)
    plt.imshow(ndvi_np, cmap='RdYlGn')
    plt.title("NDVI channel")
    plt.axis('off')

    plt.subplot(n_samples, 3, i*3 + 2)
    plt.imshow(mask_np, cmap='gray')
    plt.title("Ground Truth Mask")
    plt.axis('off')

    plt.subplot(n_samples, 3, i*3 + 3)
    plt.imshow(pred_np, cmap='gray')
    plt.title("Predicted Mask (Swin-UNet)")
    plt.axis('off')

plt.tight_layout()
plt.show()