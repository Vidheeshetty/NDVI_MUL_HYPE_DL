# -*- coding: utf-8 -*-
"""RES-UNET.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ohj9Fs_RPFElcinqAFTjvtd9Y6k0F_t-
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!apt-get install -y unzip
!pip install rasterio
!pip install torch torchvision  # Just to ensure we have PyTorch
!pip install scikit-learn      # for optional clustering if needed

import os
import glob
import numpy as np
import rasterio
from rasterio.plot import show
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

# %matplotlib inline

!apt-get install unzip
zip_path = '/content/drive/MyDrive/Agri.zip'
output_path = '/content/drive/MyDrive'

!mkdir -p {output_path}  # Create the output directory if it doesn't exist
!unzip -q "{zip_path}" -d "{output_path}"

import zipfile
# Suppose your zipped data is in 'my_data.zip' somewhere
zip_path = "/content/drive/MyDrive/Agri.zip"   # change to your actual path
output_dir = "/content/data_unzipped"

os.makedirs(output_dir, exist_ok=True)

# Unzip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(output_dir)

print("Data unzipped to:", output_dir)

import os

# Example: update this path to match your dataset
deep_dir_path = '/content/data_unzipped/Agri/Paddy/paddy_season4_RededgeMultispectral_20190917_05m/000'

os.listdir(deep_dir_path)

# Commented out IPython magic to ensure Python compatibility.
!pip install rasterio

import os
import numpy as np
import rasterio
from rasterio.plot import show
import matplotlib.pyplot as plt

# %matplotlib inline

def read_bands(directory, prefix='band', num_bands=5):
    """Reads band files from a directory and returns a stacked numpy array (bands, height, width)."""
    band_arrays = []
    for i in range(1, num_bands+1):
        band_path = os.path.join(directory, f'{prefix}{i}.tif')
        with rasterio.open(band_path) as src:
            band_arrays.append(src.read(1))  # read first (and only) band from each
    return np.stack(band_arrays, axis=0)  # shape: (5, H, W)

import os

all_files = os.listdir(deep_dir_path)
print(all_files)

import os

deep_dir_path = '/content/data_unzipped/Agri/Paddy/paddy_season4_RededgeMultispectral_20190917_05m/000'

all_files = [f for f in os.listdir(deep_dir_path) if f.endswith('.tif')]
all_files.sort()
print(all_files)

import re
from collections import defaultdict

grouped_files = defaultdict(list)

pattern = r"(IMG_\d+)_(\d+)\.tif"
# Explanation:
#  (IMG_\d+)  -> captures something like "IMG_0009"
#  _(\\d+)    -> captures the band number (1..5)
#  \.tif      -> ends with ".tif"

for filename in all_files:
    match = re.match(pattern, filename)
    if match:
        img_id = match.group(1)        # e.g. "IMG_0009"
        band_idx = match.group(2)      # e.g. "1", "2", ...
        grouped_files[img_id].append(filename)

# Check how files got grouped
for k, v in grouped_files.items():
    print(k, "->", v)

for img_id, files in grouped_files.items():
    # sort by the numeric portion of the filename
    # (which is captured as group(2) in the pattern above)
    grouped_files[img_id] = sorted(files, key=lambda f: int(re.match(pattern, f).group(2)))

# Now each group is sorted properly:
# IMG_0009 -> ['IMG_0009_1.tif', 'IMG_0009_2.tif', 'IMG_0009_3.tif', 'IMG_0009_4.tif', 'IMG_0009_5.tif']
# etc.

import rasterio
import numpy as np

def read_group_bands(directory, file_list):
    """
    Given a list of 5 .tif filenames, reads them all
    and stacks them as (bands, height, width).
    """
    band_arrays = []
    for tif_file in file_list:
        path = os.path.join(directory, tif_file)
        with rasterio.open(path) as src:
            band_arrays.append(src.read(1))  # read the single band

    return np.stack(band_arrays, axis=0)

# Example: loop through each group
for img_id, file_list in grouped_files.items():
    print(f"Reading {img_id} with files: {file_list}")
    band_stack = read_group_bands(deep_dir_path, file_list)
    print("Resulting shape:", band_stack.shape)
    # band_stack[0] = band 1
    # band_stack[1] = band 2
    # ...

def calculate_ndvi(bands, red_idx=2, nir_idx=4):
    """
    NDVI = (NIR - Red) / (NIR + Red)
    bands: shape (5, H, W)
    """
    red = bands[red_idx].astype(float)
    nir = bands[nir_idx].astype(float)
    with np.errstate(divide='ignore', invalid='ignore'):
        ndvi = (nir - red) / (nir + red)
        ndvi = np.nan_to_num(ndvi, nan=0.0, posinf=0.0, neginf=0.0)
    return ndvi

for img_id, file_list in grouped_files.items():
    band_stack = read_group_bands(deep_dir_path, file_list)
    ndvi = calculate_ndvi(band_stack, red_idx=2, nir_idx=4)
    print(f"{img_id} NDVI shape:", ndvi.shape)
    # Do something with the NDVI (plot, save, etc.)

# NDVI calculation
def calculate_ndvi(bands, red_idx=2, nir_idx=4):
    red = bands[red_idx].astype(float)
    nir = bands[nir_idx].astype(float)
    with np.errstate(divide='ignore', invalid='ignore'):
        ndvi = (nir - red) / (nir + red)
        ndvi = np.nan_to_num(ndvi, nan=0.0, posinf=0.0, neginf=0.0)
    return ndvi

# 3,4,5. Loop through groups, read, compute NDVI, etc.
for img_id, file_list in grouped_files.items():
    # Read bands into shape (5, H, W)
    bands = read_group_bands(deep_dir_path, file_list)

    # Suppose band order is:
    # 0: Blue, 1: Green, 2: Red, 3: RedEdge, 4: NIR
    ndvi_image = calculate_ndvi(bands, red_idx=2, nir_idx=4)

    # For demonstration, show the NDVI
    plt.figure(figsize=(6, 4))
    plt.title(f"{img_id} NDVI")
    plt.imshow(ndvi_image, cmap='RdYlGn')
    plt.colorbar(label='NDVI')
    plt.axis('off')
    plt.show()

def vegetation_mask(ndvi, threshold=0.6):
    """
    Returns a binary mask where pixels with NDVI >= threshold
    are considered vegetation (1), else non-vegetation (0).
    """
    mask = (ndvi >= threshold).astype(np.uint8)
    return mask

# Suppose ndvi_image is your NDVI array
veg_mask = vegetation_mask(ndvi_image, threshold=0.6)

import cv2

kernel = np.ones((3,3), np.uint8)
veg_mask_cleaned = cv2.morphologyEx(veg_mask, cv2.MORPH_CLOSE, kernel)

###########################
# STEP 3A: HELPER FUNCTIONS
###########################
import re
import cv2

def crop_black_borders(band_stack):
    """
    Crops out rows/columns where all bands are zero.
    band_stack shape: (bands, H, W)
    """
    non_zero_mask = np.any(band_stack != 0, axis=0)  # shape: (H, W)
    rows = np.any(non_zero_mask, axis=1)
    cols = np.any(non_zero_mask, axis=0)
    rmin, rmax = np.where(rows)[0][0], np.where(rows)[0][-1]
    cmin, cmax = np.where(cols)[0][0], np.where(cols)[0][-1]
    cropped_stack = band_stack[:, rmin:rmax+1, cmin:cmax+1]
    return cropped_stack

def calculate_ndvi(band_stack, red_idx=2, nir_idx=4):
    """
    NDVI = (NIR - RED) / (NIR + RED)
    band_stack shape: (5, H, W) for [Blue, Green, Red, RedEdge, NIR]
    """
    red = band_stack[red_idx].astype(float)
    nir = band_stack[nir_idx].astype(float)
    with np.errstate(divide='ignore', invalid='ignore'):
        ndvi = (nir - red) / (nir + red)
        ndvi = np.nan_to_num(ndvi, nan=0.0, posinf=0.0, neginf=0.0)
    return ndvi

###########################
# STEP 3B: GROUP BY PREFIX
###########################
all_tifs = sorted([f for f in os.listdir(deep_dir_path) if f.endswith('.tif')])

import re
from collections import defaultdict

pattern = r"(IMG_\d+)_(\d+)\.tif"
grouped_files = defaultdict(list)

for filename in all_tifs:
    match = re.match(pattern, filename)
    if match:
        img_id = match.group(1)    # e.g. "IMG_0009"
        grouped_files[img_id].append(filename)

# Sort each group by the numeric suffix (1..5)
for img_id, file_list in grouped_files.items():
    grouped_files[img_id] = sorted(file_list, key=lambda f: int(re.match(pattern, f).group(2)))

# Let's see how many sets we have
print("Number of image sets:", len(grouped_files))
for k, v in list(grouped_files.items())[:2]:
    print(k, "->", v)

###########################
# STEP 3C: PROCESS & SAVE
###########################
processed_dir = '/content/processed_data'
bands_dir = os.path.join(processed_dir, 'bands')
ndvi_dir  = os.path.join(processed_dir, 'ndvi')

os.makedirs(bands_dir, exist_ok=True)
os.makedirs(ndvi_dir, exist_ok=True)

def stack_and_save_5bands(img_id, file_list):
    """
    Reads the 5 .tif files, stacks them, crops black borders, calculates NDVI, and saves.
    """
    band_arrays = []
    for tif_file in file_list:
        path = os.path.join(deep_dir_path, tif_file)
        with rasterio.open(path) as src:
            band_arrays.append(src.read(1))  # shape: (H, W)

    band_stack = np.stack(band_arrays, axis=0)  # shape: (5, H, W)
    cropped_stack = crop_black_borders(band_stack)
    ndvi_image = calculate_ndvi(cropped_stack, red_idx=2, nir_idx=4)

    # Save stacked bands
    out_bands_path = os.path.join(bands_dir, f'{img_id}_bands.tif')

    # Copy metadata from the first band (just for georeferencing if needed)
    first_band_path = os.path.join(deep_dir_path, file_list[0])
    with rasterio.open(first_band_path) as src_ref:
        meta = src_ref.meta.copy()

    meta.update({
        'count': 5,  # 5 bands
        'width': cropped_stack.shape[2],
        'height': cropped_stack.shape[1],
        'dtype': 'float32'
    })

    with rasterio.open(out_bands_path, 'w', **meta) as dst:
        for i in range(5):
            dst.write(cropped_stack[i].astype(np.float32), i+1)

    # Save NDVI
    out_ndvi_path = os.path.join(ndvi_dir, f'{img_id}_ndvi.tif')
    meta.update({
        'count': 1,
        'dtype': 'float32'
    })
    with rasterio.open(out_ndvi_path, 'w', **meta) as dst:
        dst.write(ndvi_image.astype(np.float32), 1)

    return cropped_stack, ndvi_image

# Process each group
for img_id, files in grouped_files.items():
    cropped_bands, ndvi_img = stack_and_save_5bands(img_id, files)

print("Processing complete. Stacked 5-band and NDVI saved.")

###########################
# STEP 3D: QUICK PLOT
###########################
# Pick one example
sample_id = list(grouped_files.keys())[0]
sample_bands_path = os.path.join(bands_dir, f'{sample_id}_bands.tif')
sample_ndvi_path  = os.path.join(ndvi_dir,  f'{sample_id}_ndvi.tif')

with rasterio.open(sample_bands_path) as src:
    sample_bands = src.read()  # shape: (5, H, W)
with rasterio.open(sample_ndvi_path) as src:
    sample_ndvi = src.read(1)  # shape: (H, W)

plt.figure(figsize=(12,4))
for i in range(5):
    plt.subplot(1,6,i+1)
    plt.imshow(sample_bands[i], cmap='gray')
    plt.title(f'Band {i+1}')
    plt.axis('off')

plt.subplot(1,6,6)
plt.imshow(sample_ndvi, cmap='RdYlGn')
plt.title('NDVI')
plt.axis('off')
plt.tight_layout()
plt.show()

###########################
# STEP 4A: CREATE PSEUDO-MASKS
###########################
masks_dir = os.path.join(processed_dir, 'masks')
os.makedirs(masks_dir, exist_ok=True)

def create_pseudo_panicle_mask(band_stack, ndvi_image,
                               ndvi_min=0.2, ndvi_max=0.6,
                               rededge_thresh=300):
    """
    band_stack: shape (5, H, W) with [B, G, R, RE, NIR]
    ndvi_image: shape (H, W)
    ndvi_min, ndvi_max: NDVI range to consider potential 'panicle'
    rededge_thresh: threshold in the Red Edge band (raw units, adjust as needed)
    """
    # 1) NDVI range
    ndvi_mask = (ndvi_image >= ndvi_min) & (ndvi_image <= ndvi_max)

    # 2) RedEdge threshold
    # band_stack[3] is Red Edge if our band order is [0:Blue,1:Green,2:Red,3:RE,4:NIR]
    re_mask = (band_stack[3] >= rededge_thresh)

    combined = ndvi_mask & re_mask
    mask = combined.astype(np.uint8)

    # 3) Morphological cleanup (optional)
    kernel = np.ones((3,3), np.uint8)
    mask_cleaned = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)

    return mask_cleaned

# We'll loop through the processed bands & NDVI images to create pseudo masks
band_files = sorted(glob.glob(os.path.join(bands_dir, '*_bands.tif')))
print("Found band files for pseudo-masking:", len(band_files))

for band_path in band_files:
    filename = os.path.basename(band_path)
    img_id = filename.replace('_bands.tif','')

    ndvi_path = os.path.join(ndvi_dir, f'{img_id}_ndvi.tif')
    mask_out_path = os.path.join(masks_dir, f'{img_id}_mask.tif')

    if not os.path.exists(ndvi_path):
        continue  # skip if NDVI not found

    with rasterio.open(band_path) as src_b:
        bstack = src_b.read()  # shape: (5, H, W)
        meta_b = src_b.meta.copy()
    with rasterio.open(ndvi_path) as src_n:
        ndvi_img = src_n.read(1)
        meta_n = src_n.meta.copy()

    # Create pseudo mask
    pseudo_mask = create_pseudo_panicle_mask(
        bstack, ndvi_img, ndvi_min=0.2, ndvi_max=0.6, rededge_thresh=300
    )

    # Save mask as TIF
    meta_b.update({
        'count': 1,
        'dtype': 'uint8'
    })
    with rasterio.open(mask_out_path, 'w', **meta_b) as dst:
        dst.write(pseudo_mask, 1)

print("Pseudo masks created in:", masks_dir)

###########################
# STEP 4B: PLOT PSEUDO-MASK
###########################
sample_mask_path = os.path.join(masks_dir, f'{sample_id}_mask.tif')
if os.path.exists(sample_mask_path):
    with rasterio.open(sample_mask_path) as src_m:
        pseudo_mask = src_m.read(1)

    plt.figure(figsize=(12,4))

    plt.subplot(1,3,1)
    plt.title("Red Edge band")
    plt.imshow(sample_bands[3], cmap='gray')
    plt.axis('off')

    plt.subplot(1,3,2)
    plt.title("NDVI")
    plt.imshow(sample_ndvi, cmap='RdYlGn', vmin=-1, vmax=1)
    plt.axis('off')

    plt.subplot(1,3,3)
    plt.title("Pseudo Panicle Mask")
    plt.imshow(pseudo_mask, cmap='gray')
    plt.axis('off')

    plt.tight_layout()
    plt.show()
else:
    print("Pseudo-mask for sample not found!")

###########################
# STEP 5A: BUILD DATASET
###########################

class PaddyPseudoDataset(Dataset):
    def __init__(self, data_list):
        """
        data_list: list of tuples (band_path, ndvi_path, mask_path)
        """
        self.data_list = data_list

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        band_path, ndvi_path, mask_path = self.data_list[idx]

        with rasterio.open(band_path) as src_b:
            bands_5 = src_b.read().astype(np.float32)  # shape: (5, H, W)
        with rasterio.open(ndvi_path) as src_n:
            ndvi_img = src_n.read(1).astype(np.float32)  # shape: (H, W)
        with rasterio.open(mask_path) as src_m:
            mask_img = src_m.read(1).astype(np.float32)  # shape: (H, W)

        # Expand NDVI to (1, H, W)
        ndvi_img = np.expand_dims(ndvi_img, axis=0)

        # Combine to (6, H, W)
        image_6 = np.concatenate([bands_5, ndvi_img], axis=0)

        # Expand mask to (1, H, W)
        mask_img = np.expand_dims(mask_img, axis=0)

        # Convert to torch
        image_6 = torch.from_numpy(image_6)
        mask_img = torch.from_numpy(mask_img)

        return image_6, mask_img

# Collect all matched files
band_paths = sorted(glob.glob(os.path.join(bands_dir, '*_bands.tif')))
data_list = []
for bp in band_paths:
    filename = os.path.basename(bp)
    img_id = filename.replace('_bands.tif','')
    np_path = os.path.join(ndvi_dir, f'{img_id}_ndvi.tif')
    mk_path = os.path.join(masks_dir, f'{img_id}_mask.tif')
    if os.path.exists(np_path) and os.path.exists(mk_path):
        data_list.append((bp, np_path, mk_path))

print("Total dataset samples:", len(data_list))

# Split train/val/test
trainval_list, test_list = train_test_split(data_list, test_size=0.2, random_state=42)
train_list, val_list = train_test_split(trainval_list, test_size=0.25, random_state=42)
# => 60% train, 20% val, 20% test

train_dataset = PaddyPseudoDataset(train_list)
val_dataset   = PaddyPseudoDataset(val_list)
test_dataset  = PaddyPseudoDataset(test_list)

print("Train:", len(train_dataset), "Val:", len(val_dataset), "Test:", len(test_dataset))

band_files = sorted(glob.glob(os.path.join(bands_dir, "*_bands.tif")))
data_list = []
for bp in band_files:
    base_id = os.path.basename(bp).replace("_bands.tif","")
    ndvi_p  = os.path.join(ndvi_dir,  f"{base_id}_ndvi.tif")
    mk_p    = os.path.join(masks_dir, f"{base_id}_mask.tif")
    if os.path.exists(ndvi_p) and os.path.exists(mk_p):
        data_list.append((bp, ndvi_p, mk_p))

print("Data samples:", len(data_list))

from sklearn.model_selection import train_test_split

trainval_list, test_list = train_test_split(data_list, test_size=0.2, random_state=42)
train_list, val_list     = train_test_split(trainval_list, test_size=0.25, random_state=42)
print(f"Train: {len(train_list)}, Val: {len(val_list)}, Test: {len(test_list)}")

import torch
from torch.utils.data import Dataset, DataLoader
import cv2

class PaddyPseudoDataset(Dataset):
    def __init__(self, samples, resize_hw=(256,256)):
        self.samples = samples
        self.resize_hw = resize_hw

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        b_path, n_path, m_path = self.samples[idx]

        with rasterio.open(b_path) as sb:
            bands_5 = sb.read().astype(np.float32)  # (5, H, W)
        with rasterio.open(n_path) as sn:
            ndvi_img = sn.read(1).astype(np.float32) # (H, W)
        ndvi_img = np.expand_dims(ndvi_img, axis=0)  # => (1,H,W)

        image_6 = np.concatenate([bands_5, ndvi_img], axis=0)  # => (6,H,W)

        with rasterio.open(m_path) as sm:
            mask_img = sm.read(1).astype(np.float32)  # => (H,W)
        mask_img = np.expand_dims(mask_img, axis=0)  # => (1,H,W)

        if self.resize_hw is not None:
            resized_chans = []
            for i in range(6):
                ch = image_6[i]
                ch_res = cv2.resize(ch, (self.resize_hw[1], self.resize_hw[0]), interpolation=cv2.INTER_AREA)
                resized_chans.append(ch_res)
            image_6 = np.stack(resized_chans, axis=0)

            mk_res = cv2.resize(mask_img[0], (self.resize_hw[1], self.resize_hw[0]),
                                interpolation=cv2.INTER_NEAREST)
            mask_img = np.expand_dims(mk_res, axis=0)

        image_6 = torch.from_numpy(image_6)
        mask_img = torch.from_numpy(mask_img)
        return image_6, mask_img

train_dataset = PaddyPseudoDataset(train_list, (256,256))
val_dataset   = PaddyPseudoDataset(val_list,   (256,256))
test_dataset  = PaddyPseudoDataset(test_list,  (256,256))

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)
val_loader   = DataLoader(val_dataset,   batch_size=8, shuffle=False, num_workers=0)
test_loader  = DataLoader(test_dataset,  batch_size=16, shuffle=False, num_workers=0)

import torch
import torch.nn as nn
import torch.nn.functional as F

# -- Basic building blocks: Patch Embedding, Swin blocks, etc.

class PatchEmbed(nn.Module):
    """
    Embeds the input images by splitting them into patches and applying a linear embedding.
    We adapt it to handle 6 channels instead of 3.
    """
    def __init__(self, patch_size=4, in_chans=6, embed_dim=96):
        super().__init__()
        self.patch_size = patch_size
        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim,
                              kernel_size=patch_size,
                              stride=patch_size)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        # x: (B, 6, H, W)
        B, C, H, W = x.shape
        # reduce size => (B, embed_dim, H/patch, W/patch)
        x = self.proj(x)
        # flatten => (B, embed_dim, num_patches)
        x = x.flatten(2).transpose(1, 2)  # => (B, nPatches, embed_dim)
        x = self.norm(x)
        return x

# We would define SwinTransformerBlock, SwinStage, etc.
# For brevity, we keep it short. We'll assume we have a "SwinEncoder" that returns
# multiple feature maps for U-Net style decoder.

class DummySwinEncoder(nn.Module):
    """
    A placeholder 'Swin' encoder that returns feature maps at 4 levels.
    (In a real scenario, you'd implement the Swin blocks or import from a library.)
    """
    def __init__(self, in_chans=6, embed_dim=96):
        super().__init__()
        self.patch_embed = PatchEmbed(patch_size=4, in_chans=in_chans, embed_dim=embed_dim)

        # We'll just simulate 4 stages with linear layers.
        self.stage1 = nn.Linear(embed_dim, embed_dim)
        self.stage2 = nn.Linear(embed_dim, embed_dim*2)
        self.stage3 = nn.Linear(embed_dim*2, embed_dim*4)
        self.stage4 = nn.Linear(embed_dim*4, embed_dim*8)

        # The real Swin would have windowed MSA, shifting, etc.

    def forward(self, x):
        """
        x: (B,6,H,W)
        We'll produce 4 feature maps (like a U-Net encoder)
        but in practice these are 1D embeddings we might reshape back.
        For demonstration, we'll just show dimension transformations.
        """
        # patch embed
        x = self.patch_embed(x)   # => (B, nPatches, embed_dim)
        B, N, C = x.shape

        # Stage1
        x1 = self.stage1(x)  # => (B,N,embed_dim)
        # Stage2
        x2 = self.stage2(x1) # => (B,N,2*embed_dim)
        # Stage3
        x3 = self.stage3(x2) # => (B,N,4*embed_dim)
        # Stage4
        x4 = self.stage4(x3) # => (B,N,8*embed_dim)

        # For "U-Net" style, we'd reshape each stage into (B,Channels,H',W')
        # We'll just store them as flattened for demonstration.
        return x1, x2, x3, x4

# The decoder would upsample these 4 features back to the original size

class DummySwinUNetDecoder(nn.Module):
    """
    Very simplistic decoder that fakes some upsampling steps.
    In a real approach, you'd reshape the tokens back to spatial,
    then do skip connections, upsample, etc.
    """
    def __init__(self, embed_dim=96, out_channels=1):
        super().__init__()
        # We'll define a few linear layers to simulate "decoding".
        # A real approach would have pixel shuffle or token rearrangements.
        self.linear4 = nn.Linear(embed_dim*8, embed_dim*4)
        self.linear3 = nn.Linear(embed_dim*4, embed_dim*2)
        self.linear2 = nn.Linear(embed_dim*2, embed_dim)
        self.linear1 = nn.Linear(embed_dim, out_channels)

    def forward(self, feats):
        # feats = (x1, x2, x3, x4)
        x1, x2, x3, x4 = feats
        # Start from x4
        B, N, C = x4.shape
        d4 = self.linear4(x4)  # => (B,N,4*embed_dim)
        d3 = self.linear3(d4)  # => (B,N,2*embed_dim)
        d2 = self.linear2(d3)  # => (B,N,embed_dim)
        d1 = self.linear1(d2)  # => (B,N,out_channels)

        # Suppose we reshape back to 2D (dummy approach)
        # We know N= (H/4) * (W/4) if patch_size=4, roughly
        # We'll just do a rough guess. This is very simplified.

        # Output shape => (B, out_channels, H, W)
        # Real Swin-UNet code is more involved. This is purely illustrative.

        return d1

class SwinUNet6ch(nn.Module):
    """
    A highly simplified "Swin-UNet" style model adapted for 6 input channels.
    In practice, you'd use a real implementation with token rearrangements, skip merges, etc.
    """
    def __init__(self, in_chans=6, out_ch=1, embed_dim=96):
        super().__init__()
        self.encoder = DummySwinEncoder(in_chans=in_chans, embed_dim=embed_dim)
        self.decoder = DummySwinUNetDecoder(embed_dim=embed_dim, out_channels=out_ch)

    def forward(self, x):
        # Encode => 4 feature maps
        feats = self.encoder(x)  # (x1,x2,x3,x4)
        # Decode => final output
        out = self.decoder(feats)  # shape => (B,N,out_ch)
        # We'll do a quick reshape guess to (B,out_ch,H,W).
        B, N, C = out.shape
        # Suppose our input was (B,6,H,W), patch_size=4 => N ~ (H/4 * W/4).
        # We can't do an exact reshape without storing H/W.
        # We'll store them from x in the forward call. Let's do that.

        return out  # (B,N, out_ch) => a 2D restructure is needed for real usage

class PatchEmbed(nn.Module):
    def __init__(self, patch_size=4, in_chans=6, embed_dim=96):
        super().__init__()
        self.patch_size = patch_size
        self.in_chans   = in_chans
        self.embed_dim  = embed_dim
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.norm = nn.LayerNorm(embed_dim)
        self.H = None
        self.W = None

    def forward(self, x):
        B, C, H, W = x.shape
        self.H, self.W = H, W
        # # patch embed
        x = self.proj(x)   # => (B,embed_dim,H/patch,W/patch)
        _, E, Hp, Wp = x.shape
        x = x.flatten(2).transpose(1,2)  # => (B,Hp*Wp, E)
        x = self.norm(x)
        return x, (Hp, Wp)

# Then update DummySwinEncoder and DummySwinUNetDecoder to handle shapes for a real "Swin-UNet."

# For brevity, let's keep the concept. We won't rewrite the entire architecture in detail,
# but assume we can produce a (B, out_ch, H, W) final output.

import torch
import torch.nn as nn
import torch.nn.functional as F

class PatchEmbed(nn.Module):
    """
    Embeds the input by splitting into patches (patch_size x patch_size),
    for in_chans=6, then projects to 'embed_dim' channels.

    Returns:
      x: (B, num_patches, embed_dim)
      (Hp, Wp): the patch-grid size
    """
    def __init__(self, patch_size=4, in_chans=6, embed_dim=96):
        super().__init__()
        self.patch_size = patch_size
        self.in_chans   = in_chans
        self.embed_dim  = embed_dim

        # Convolution to get embeddings
        self.proj = nn.Conv2d(in_chans, embed_dim,
                              kernel_size=patch_size,
                              stride=patch_size)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        # x: (B,6,H,W)
        B, C, H, W = x.shape
        # convolving => shape: (B,embed_dim,H//patch_size, W//patch_size)
        x = self.proj(x)
        # flatten => (B, embed_dim, num_patches) => (B, num_patches, embed_dim)
        B, E, Hp, Wp = x.shape
        x = x.flatten(2).transpose(1,2)  # => (B, Hp*Wp, E)
        x = self.norm(x)
        return x, (Hp, Wp)

class DummySwinEncoder(nn.Module):
    """
    A placeholder "Swin" encoder that returns four feature maps in embeddings form.
    In reality, Swin Transformer blocks do more complicated steps.
    """
    def __init__(self, in_chans=6, embed_dim=96):
        super().__init__()
        self.patch_embed = PatchEmbed(patch_size=4, in_chans=in_chans, embed_dim=embed_dim)

        # We'll define four linear transformations to simulate 4 stages
        self.stage1 = nn.Linear(embed_dim, embed_dim)       # (B,N, embed_dim)
        self.stage2 = nn.Linear(embed_dim, embed_dim*2)     # ...
        self.stage3 = nn.Linear(embed_dim*2, embed_dim*4)
        self.stage4 = nn.Linear(embed_dim*4, embed_dim*8)

    def forward(self, x):
        # patch embedding => x_emb, (Hp, Wp)
        x_emb, (Hp, Wp) = self.patch_embed(x)  # x_emb => (B, N, embed_dim)

        # Stage1
        x1 = self.stage1(x_emb)  # => (B,N, embed_dim)
        # Stage2
        x2 = self.stage2(x1)     # => (B,N, embed_dim*2)
        # Stage3
        x3 = self.stage3(x2)     # => (B,N, embed_dim*4)
        # Stage4
        x4 = self.stage4(x3)     # => (B,N, embed_dim*8)

        return (x1, x2, x3, x4), (Hp, Wp)
        # We'll also return (Hp, Wp) so the decoder can reshape accordingly

class DummySwinUNetDecoder(nn.Module):
    """
    Minimal: We'll just take x4 as the main feature, do some linear transforms,
    then upsample to (H, W).
    """
    def __init__(self, embed_dim=96, out_channels=1):
        super().__init__()
        # Let's say x4 has embed_dim*8 channels
        self.linear_4toFinal = nn.Linear(embed_dim*8, out_channels)  # final channel dimension

    def forward(self, feats, patch_hw, patch_size=4):
        """
        feats: (x1, x2, x3, x4) each => (B,N,Channels)
        patch_hw: (Hp, Wp)
        """
        x1, x2, x3, x4 = feats
        Hp, Wp = patch_hw  # the patch-grid dimension

        # We'll just use x4 for demonstration
        B, N, C = x4.shape
        # linear reduce from embed_dim*8 => out_channels
        out_flat = self.linear_4toFinal(x4)  # => (B, N, out_channels)

        # now reshape to (B, out_channels, Hp, Wp)
        out_flat = out_flat.permute(0,2,1)  # => (B, out_channels, N)
        out_2d = out_flat.reshape(B, -1, Hp, Wp)

        # final upsample by patch_size => (B, out_channels, patch_size*Hp, patch_size*Wp)
        out_img = F.interpolate(out_2d, scale_factor=patch_size, mode='bilinear', align_corners=False)

        return out_img

class SwinUNet6ch(nn.Module):
    """
    Minimal Swin-UNet for 6 channels.
    """
    def __init__(self, in_chans=6, out_ch=1, embed_dim=96):
        super().__init__()
        self.encoder = DummySwinEncoder(in_chans=in_chans, embed_dim=embed_dim)
        self.decoder = DummySwinUNetDecoder(embed_dim=embed_dim, out_channels=out_ch)

    def forward(self, x):
        # encode => feats, (Hp, Wp)
        feats, (Hp, Wp) = self.encoder(x)  # feats => (x1,x2,x3,x4)
        # decode => (B, out_ch, H, W)
        out = self.decoder(feats, (Hp, Wp), patch_size=4)
        return out

import gc
import torch.nn as nn
import torch.optim as optim

def dice_coeff(pred, target, smooth=1e-5):
    pred_bin = (torch.sigmoid(pred) > 0.5).float()
    intersection = (pred_bin * target).sum(dim=(1,2,3))
    union = pred_bin.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))
    dice = (2.0 * intersection + smooth) / (union + smooth)
    return dice.mean()

def train_one_epoch_swin(model, loader, optimizer, criterion):
    model.train()
    epoch_loss = 0.0
    epoch_dice = 0.0
    for images, masks in loader:
        optimizer.zero_grad()
        outputs = model(images)  # => (B, out_ch, H, W)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        epoch_dice += dice_coeff(outputs, masks).item()

        del images, masks, outputs, loss
        gc.collect()

    return epoch_loss / len(loader), epoch_dice / len(loader)

def validate_one_epoch_swin(model, loader, criterion):
    model.eval()
    val_loss = 0.0
    val_dice = 0.0
    with torch.no_grad():
        for images, masks in loader:
            outputs = model(images)
            loss = criterion(outputs, masks)

            val_loss += loss.item()
            val_dice += dice_coeff(outputs, masks).item()

            del images, masks, outputs, loss
            gc.collect()
    return val_loss / len(loader), val_dice / len(loader)

# Instantiate
model_swin = SwinUNet6ch(in_chans=6, out_ch=1, embed_dim=96)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model_swin.parameters(), lr=1e-4)
num_epochs = 100

train_loss_history = []
train_dice_history = []
val_loss_history   = []
val_dice_history   = []

for epoch in range(num_epochs):
    tr_loss, tr_dice = train_one_epoch_swin(model_swin, train_loader, optimizer, criterion)
    va_loss, va_dice = validate_one_epoch_swin(model_swin, val_loader, criterion)

    train_loss_history.append(tr_loss)
    train_dice_history.append(tr_dice)
    val_loss_history.append(va_loss)
    val_dice_history.append(va_dice)

    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"| Train Loss: {tr_loss:.4f}, Train Dice: {tr_dice:.4f} "
          f"| Val Loss: {va_loss:.4f}, Val Dice: {va_dice:.4f}")

###########################
# STEP 5B: UNET MODEL
###########################
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    def forward(self, x):
        return self.conv(x)

class UNet(nn.Module):
    def __init__(self, in_channels=6, out_channels=1):
        super().__init__()
        self.dc1 = DoubleConv(in_channels, 64)
        self.dc2 = DoubleConv(64, 128)
        self.dc3 = DoubleConv(128, 256)
        self.dc4 = DoubleConv(256, 512)
        self.pool = nn.MaxPool2d(2)

        self.bottleneck = DoubleConv(512, 1024)

        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.dc_up4 = DoubleConv(1024, 512)

        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dc_up3 = DoubleConv(512, 256)

        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dc_up2 = DoubleConv(256, 128)

        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dc_up1 = DoubleConv(128, 64)

        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)

    def forward(self, x):
        # Encoder
        c1 = self.dc1(x)
        p1 = self.pool(c1)
        c2 = self.dc2(p1)
        p2 = self.pool(c2)
        c3 = self.dc3(p2)
        p3 = self.pool(c3)
        c4 = self.dc4(p3)
        p4 = self.pool(c4)

        # Bottleneck
        cb = self.bottleneck(p4)

        # Decoder
        u4 = self.up4(cb)
        cat4 = torch.cat([u4, c4], dim=1)
        c4_up = self.dc_up4(cat4)

        u3 = self.up3(c4_up)
        cat3 = torch.cat([u3, c3], dim=1)
        c3_up = self.dc_up3(cat3)

        u2 = self.up2(c3_up)
        cat2 = torch.cat([u2, c2], dim=1)
        c2_up = self.dc_up2(cat2)

        u1 = self.up1(c2_up)
        cat1 = torch.cat([u1, c1], dim=1)
        c1_up = self.dc_up1(cat1)

        out = self.out_conv(c1_up)
        return out  # shape: (batch, 1, H, W)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import gc
import matplotlib.pyplot as plt

###########################
# (A) Define DoubleConv and UNet
###########################
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.conv(x)

class UNet(nn.Module):
    def __init__(self, in_channels=6, out_channels=1):
        """
        in_channels = 6 => 5 bands + NDVI
        out_channels = 1 => binary segmentation
        """
        super().__init__()
        self.dc1 = DoubleConv(in_channels, 64)
        self.dc2 = DoubleConv(64, 128)
        self.dc3 = DoubleConv(128, 256)
        self.dc4 = DoubleConv(256, 512)
        self.pool = nn.MaxPool2d(2)

        self.bottleneck = DoubleConv(512, 1024)

        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.dc_up4 = DoubleConv(1024, 512)

        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dc_up3 = DoubleConv(512, 256)

        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dc_up2 = DoubleConv(256, 128)

        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dc_up1 = DoubleConv(128, 64)

        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)

    def forward(self, x):
        # Encoder
        c1 = self.dc1(x)
        p1 = self.pool(c1)
        c2 = self.dc2(p1)
        p2 = self.pool(c2)
        c3 = self.dc3(p2)
        p3 = self.pool(c3)
        c4 = self.dc4(p3)
        p4 = self.pool(c4)

        # Bottleneck
        cb = self.bottleneck(p4)

        # Decoder
        u4 = self.up4(cb)
        cat4 = torch.cat([u4, c4], dim=1)
        c4_up = self.dc_up4(cat4)

        u3 = self.up3(c4_up)
        cat3 = torch.cat([u3, c3], dim=1)
        c3_up = self.dc_up3(cat3)

        u2 = self.up2(c3_up)
        cat2 = torch.cat([u2, c2], dim=1)
        c2_up = self.dc_up2(cat2)

        u1 = self.up1(c2_up)
        cat1 = torch.cat([u1, c1], dim=1)
        c1_up = self.dc_up1(cat1)

        out = self.out_conv(c1_up)
        return out

###########################
# (B) Dice Coefficient
###########################
def dice_coeff(pred, target, smooth=1e-5):
    """
    pred, target: (N, 1, H, W)
    Apply sigmoid to `pred`, threshold at 0.5, then compute Dice.
    """
    pred_bin = (torch.sigmoid(pred) > 0.5).float()
    intersection = (pred_bin * target).sum(dim=(1,2,3))
    union = pred_bin.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))
    dice = (2.0 * intersection + smooth) / (union + smooth)
    return dice.mean()

###########################
# (C) Train & Validate Functions
###########################
def train_one_epoch(model, loader, optimizer, criterion):
    model.train()
    epoch_loss = 0.0
    epoch_dice = 0.0

    for images, masks in loader:
        # Forward
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)

        # Backward
        loss.backward()
        optimizer.step()

        # Metrics
        epoch_loss += loss.item()
        epoch_dice += dice_coeff(outputs, masks).item()

        # Memory cleanup
        del images, masks, outputs, loss
        gc.collect()

    return epoch_loss / len(loader), epoch_dice / len(loader)

def validate_one_epoch(model, loader, criterion):
    model.eval()
    val_loss = 0.0
    val_dice = 0.0

    with torch.no_grad():
        for images, masks in loader:
            outputs = model(images)
            loss = criterion(outputs, masks)

            val_loss += loss.item()
            val_dice += dice_coeff(outputs, masks).item()

            # Memory cleanup
            del images, masks, outputs, loss
            gc.collect()

    return val_loss / len(loader), val_dice / len(loader)

###########################
# (D) Instantiate Datasets & Loaders
###########################
# We assume you already have:
#   train_dataset, val_dataset
# each created from your custom code or from steps 5B and earlier.

# Key memory-saving changes:
#   - batch_size=2 (or 1)
#   - num_workers=0
#   - no device references => CPU only
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)
val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, num_workers=0)
test_loader   = DataLoader(test_dataset,   batch_size=32, shuffle=False, num_workers=0)

###########################
# (E) Define Model, Loss, Optimizer
###########################
model = UNet(in_channels=6, out_channels=1)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

###########################
# (F) Training Loop
###########################
num_epochs = 10

train_loss_history = []
train_dice_history = []
val_loss_history   = []
val_dice_history   = []

for epoch in range(num_epochs):
    train_loss, train_dice = train_one_epoch(model, train_loader, optimizer, criterion)
    val_loss, val_dice = validate_one_epoch(model, val_loader, criterion)

    # Record metrics
    train_loss_history.append(train_loss)
    train_dice_history.append(train_dice)
    val_loss_history.append(val_loss)
    val_dice_history.append(val_dice)

    # Print status
    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f} | "
          f"Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}")

# Plot final learning curves
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(train_loss_history, label='Train Loss')
plt.plot(val_loss_history,   label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss over Epochs')
plt.legend()

plt.subplot(1,2,2)
plt.plot(train_dice_history, label='Train Dice')
plt.plot(val_dice_history,   label='Val Dice')
plt.xlabel('Epoch')
plt.ylabel('Dice')
plt.title('Dice Coefficient over Epochs')
plt.legend()

plt.show()

# Plot final learning curves
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(train_loss_history, label='Train Loss')
plt.plot(val_loss_history,   label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss over Epochs')
plt.legend()

plt.subplot(1,2,2)
plt.plot(train_dice_history, label='Train Dice')
plt.plot(val_dice_history,   label='Val Dice')
plt.xlabel('Epoch')
plt.ylabel('Dice')
plt.title('Dice Coefficient over Epochs')
plt.legend()

plt.show()

test_loss = 0.0
test_dice = 0.0
model.eval()
with torch.no_grad():
    for images, masks in test_loader:
        outputs = model(images)
        loss = criterion(outputs, masks)
        test_loss += loss.item()
        test_dice += dice_coeff(outputs, masks).item()

        del images, masks, outputs, loss
        gc.collect()

test_loss /= len(test_loader)
test_dice /= len(test_loader)
print(f"Test Loss: {test_loss:.4f}, Test Dice: {test_dice:.4f}")

import random

n_samples_to_show = 3
model.eval()
indices = random.sample(range(len(test_dataset)), n_samples_to_show)

plt.figure(figsize=(12, 4*n_samples_to_show))
for i, idx in enumerate(indices):
    image_6, mask_gt = test_dataset[idx]  # shape (6, H, W), (1, H, W)
    # Convert to batch dimension
    image_6 = image_6.unsqueeze(0)

    with torch.no_grad():
        pred = model(image_6)  # (1, 1, H, W)
    pred_sigmoid = torch.sigmoid(pred)
    pred_bin = (pred_sigmoid > 0.5).float()

    # Convert to numpy
    pred_bin_np = pred_bin.squeeze().numpy()
    mask_gt_np = mask_gt.squeeze().numpy()
    ndvi_np = image_6.squeeze()[5].numpy()  # 6th channel is NDVI if your band order is [0..4=5bands,5=NDVI]

    plt.subplot(n_samples_to_show, 3, i*3 + 1)
    plt.title("NDVI Channel")
    plt.imshow(ndvi_np, cmap='RdYlGn')
    plt.axis('off')

    plt.subplot(n_samples_to_show, 3, i*3 + 2)
    plt.title("Ground Truth Mask")
    plt.imshow(mask_gt_np, cmap='gray')
    plt.axis('off')

    plt.subplot(n_samples_to_show, 3, i*3 + 3)
    plt.title("Predicted Mask")
    plt.imshow(pred_bin_np, cmap='gray')
    plt.axis('off')

plt.tight_layout()
plt.show()

# Assuming 'model' is your trained PyTorch model
torch.save(model.state_dict(), 'paniclepaddy80.pth')

"""### RESUNET"""

import numpy as np
import rasterio
import matplotlib.pyplot as plt
import cv2

def crop_black_borders(band_stack):
    """
    band_stack shape: (5, H, W)
    Removes zero-only rows/cols.
    """
    non_zero_mask = np.any(band_stack != 0, axis=0)
    rows = np.any(non_zero_mask, axis=1)
    cols = np.any(non_zero_mask, axis=0)
    rmin, rmax = np.where(rows)[0][0], np.where(rows)[0][-1]
    cmin, cmax = np.where(cols)[0][0], np.where(cols)[0][-1]
    return band_stack[:, rmin:rmax+1, cmin:cmax+1]

def calculate_ndvi(band_stack, red_idx=2, nir_idx=4):
    """
    NDVI = (NIR - RED)/(NIR + RED)
    band order: [B=0, G=1, R=2, RE=3, NIR=4].
    """
    red = band_stack[red_idx].astype(np.float32)
    nir = band_stack[nir_idx].astype(np.float32)
    with np.errstate(divide='ignore', invalid='ignore'):
        ndvi = (nir - red) / (nir + red)
        ndvi = np.nan_to_num(ndvi, nan=0.0, posinf=0.0, neginf=0.0)
    return ndvi

preprocessed_data = {}

for img_id, files in grouped_files.items():
    band_arrays = []
    for tif_name in files:
        path = os.path.join(deep_dir_path, tif_name)
        with rasterio.open(path) as src:
            band_arrays.append(src.read(1))
    band_stack = np.stack(band_arrays, axis=0)  # (5,H,W)
    cropped = crop_black_borders(band_stack)
    ndvi_img = calculate_ndvi(cropped, red_idx=2, nir_idx=4)
    preprocessed_data[img_id] = (cropped, ndvi_img)

sample_id = list(preprocessed_data.keys())[0]
bands_5, sample_ndvi = preprocessed_data[sample_id]
print("Sample shape (5-band cropped):", bands_5.shape)
print("Sample NDVI shape:", sample_ndvi.shape)

# Plot
plt.figure(figsize=(12,4))
for i in range(5):
    plt.subplot(1,6,i+1)
    plt.imshow(bands_5[i], cmap='gray')
    plt.title(f"Band {i+1}")
    plt.axis('off')

plt.subplot(1,6,6)
plt.imshow(sample_ndvi, cmap='RdYlGn')
plt.title("NDVI")
plt.axis('off')
plt.tight_layout()
plt.show()

def pseudo_panicle_mask(bands_5, ndvi_img,
                        ndvi_min=0.2, ndvi_max=0.6,
                        rededge_idx=3, rededge_thresh=300):
    """
    Example:
      NDVI in [0.2, 0.6]
      RedEdge >= 300
    """
    ndvi_mask = (ndvi_img >= ndvi_min) & (ndvi_img <= ndvi_max)
    re_mask   = (bands_5[rededge_idx] >= rededge_thresh)
    combined  = ndvi_mask & re_mask
    mask_bin  = combined.astype(np.uint8)

    kernel = np.ones((3,3), np.uint8)
    mask_clean = cv2.morphologyEx(mask_bin, cv2.MORPH_OPEN, kernel)
    return mask_clean

pseudo_data = {}  # store (bands_5, ndvi_img, pseudo_mask)
for img_id, (b5, ndvi_img) in preprocessed_data.items():
    pmask = pseudo_panicle_mask(b5, ndvi_img)
    pseudo_data[img_id] = (b5, ndvi_img, pmask)

# Plot sample
b5_samp, ndvi_samp, pmask_samp = pseudo_data[sample_id]
plt.figure(figsize=(12,4))
plt.subplot(1,3,1)
plt.imshow(b5_samp[3], cmap='gray')
plt.title("RedEdge band")
plt.axis('off')

plt.subplot(1,3,2)
plt.imshow(ndvi_samp, cmap='RdYlGn')
plt.title("NDVI")
plt.axis('off')

plt.subplot(1,3,3)
plt.imshow(pmask_samp, cmap='gray')
plt.title("Pseudo Mask")
plt.axis('off')
plt.tight_layout()
plt.show()

dataset_list = []
for img_id, (b5, ndvi_img, pmask) in pseudo_data.items():
    dataset_list.append((img_id, b5, ndvi_img, pmask))

print("Total dataset samples:", len(dataset_list))

from sklearn.model_selection import train_test_split
trainval_list, test_list = train_test_split(dataset_list, test_size=0.2, random_state=42)
train_list, val_list = train_test_split(trainval_list, test_size=0.25, random_state=42)
print(f"Train: {len(train_list)}, Val: {len(val_list)}, Test: {len(test_list)}")

import torch
from torch.utils.data import Dataset, DataLoader
import cv2

class PaniclePseudoDataset(Dataset):
    """
    returns X: (6, H, W) => (5-band + NDVI),
            Y: (1, H, W) => pseudo mask
    with optional resizing
    """
    def __init__(self, samples, resize_hw=(256,256)):
        self.samples = samples
        self.resize_hw = resize_hw

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_id, b5, ndvi_img, pmask = self.samples[idx]
        # b5 shape => (5,H,W), ndvi =>(H,W), pmask= (H,W)
        ndvi_img = np.expand_dims(ndvi_img, axis=0)  # =>(1,H,W)
        image_6 = np.concatenate([b5, ndvi_img], axis=0) # =>(6,H,W)

        pmask = np.expand_dims(pmask, axis=0)  # =>(1,H,W)

        if self.resize_hw is not None:
            resized_channels = []
            for i in range(6):
                ch = image_6[i]
                ch_res = cv2.resize(ch, (self.resize_hw[1], self.resize_hw[0]),
                                    interpolation=cv2.INTER_AREA)
                resized_channels.append(ch_res)
            image_6 = np.stack(resized_channels, axis=0)

            mk_res = cv2.resize(pmask[0], (self.resize_hw[1], self.resize_hw[0]),
                                interpolation=cv2.INTER_NEAREST)
            pmask = np.expand_dims(mk_res, axis=0)

        image_6 = torch.from_numpy(image_6).float()   # => (6,H,W)
        pmask = torch.from_numpy(pmask).float()       # => (1,H,W)

        return image_6, pmask

train_dataset = PaniclePseudoDataset(train_list, (256,256))
val_dataset   = PaniclePseudoDataset(val_list,   (256,256))
test_dataset  = PaniclePseudoDataset(test_list,  (256,256))

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)
val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, num_workers=0)
test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False, num_workers=0)

import torch.nn as nn

class ResBlock(nn.Module):
    """
    A 2-conv residual block with BN + ReLU.
    in_channels -> out_channels
    """
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)

        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # if in_channels != out_channels, we do a 1x1 conv to match dims
        self.skip = None
        if in_channels != out_channels:
            self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        # residual
        if self.skip is not None:
            identity = self.skip(identity)

        out += identity
        out = self.relu(out)
        return out

class ResUNet(nn.Module):
    """
    U-Net with residual blocks in the encoder & decoder.
    For 6-channel input, out_channels=1 for binary seg.
    """
    def __init__(self, in_channels=6, out_channels=1):
        super().__init__()
        # Encoder
        self.enc1 = ResBlock(in_channels, 64)
        self.pool1 = nn.MaxPool2d(2)

        self.enc2 = ResBlock(64, 128)
        self.pool2 = nn.MaxPool2d(2)

        self.enc3 = ResBlock(128, 256)
        self.pool3 = nn.MaxPool2d(2)

        self.enc4 = ResBlock(256, 512)
        self.pool4 = nn.MaxPool2d(2)

        # Bottleneck
        self.bottleneck = ResBlock(512, 1024)

        # Decoder
        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.dec4 = ResBlock(1024, 512)

        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dec3 = ResBlock(512, 256)

        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec2 = ResBlock(256, 128)

        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec1 = ResBlock(128, 64)

        # final conv
        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)

    def forward(self, x):
        # Encoder
        e1 = self.enc1(x)  # => 64
        p1 = self.pool1(e1)

        e2 = self.enc2(p1) # => 128
        p2 = self.pool2(e2)

        e3 = self.enc3(p2) # => 256
        p3 = self.pool3(e3)

        e4 = self.enc4(p3) # => 512
        p4 = self.pool4(e4)

        # Bottleneck
        bn = self.bottleneck(p4) # => 1024

        # Decoder
        u4 = self.up4(bn)        # => 512
        c4 = torch.cat([u4, e4], dim=1) # skip
        d4 = self.dec4(c4)

        u3 = self.up3(d4)
        c3 = torch.cat([u3, e3], dim=1)
        d3 = self.dec3(c3)

        u2 = self.up2(d3)
        c2 = torch.cat([u2, e2], dim=1)
        d2 = self.dec2(c2)

        u1 = self.up1(d2)
        c1 = torch.cat([u1, e1], dim=1)
        d1 = self.dec1(c1)

        out = self.out_conv(d1)  # => (B,1,H,W)
        return out

import torch
import torch.nn as nn
import torch.optim as optim
import gc

def dice_coeff(pred, target, smooth=1e-5):
    """
    pred, target => (N,1,H,W)
    binarize pred with 0.5 threshold
    """
    pred_bin = (torch.sigmoid(pred) > 0.5).float()
    intersection = (pred_bin * target).sum(dim=(1,2,3))
    union = pred_bin.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))
    dice = (2.0 * intersection + smooth) / (union + smooth)
    return dice.mean()

def iou_coeff(pred, target, smooth=1e-5):
    """
    IoU = intersection / union
    """
    pred_bin = (torch.sigmoid(pred) > 0.5).float()
    intersection = (pred_bin * target).sum(dim=(1,2,3))
    union = (pred_bin + target).sum(dim=(1,2,3)) - intersection
    iou = (intersection + smooth) / (union + smooth)
    return iou.mean()

def train_one_epoch(model, loader, optimizer, criterion):
    model.train()
    epoch_loss = 0.0
    epoch_dice = 0.0
    epoch_iou  = 0.0

    for images, masks in loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        d = dice_coeff(outputs, masks).item()
        i = iou_coeff(outputs, masks).item()

        epoch_loss += loss.item()
        epoch_dice += d
        epoch_iou  += i

        del images, masks, outputs, loss
        gc.collect()

    n = len(loader)
    return epoch_loss/n, epoch_dice/n, epoch_iou/n

def validate_one_epoch(model, loader, criterion):
    model.eval()
    val_loss = 0.0
    val_dice = 0.0
    val_iou  = 0.0

    with torch.no_grad():
        for images, masks in loader:
            outputs = model(images)
            loss = criterion(outputs, masks)

            d = dice_coeff(outputs, masks).item()
            i = iou_coeff(outputs, masks).item()

            val_loss += loss.item()
            val_dice += d
            val_iou  += i

            del images, masks, outputs, loss
            gc.collect()

    n = len(loader)
    return val_loss/n, val_dice/n, val_iou/n

# Instantiate model, loss, optimizer
model_resunet = ResUNet(in_channels=6, out_channels=1)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model_resunet.parameters(), lr=1e-4)

num_epochs = 10  # Increase for better results

train_loss_history = []
train_dice_history = []
train_iou_history  = []
val_loss_history   = []
val_dice_history   = []
val_iou_history    = []

for epoch in range(num_epochs):
    tr_loss, tr_dice, tr_iou = train_one_epoch(model_resunet, train_loader, optimizer, criterion)
    va_loss, va_dice, va_iou = validate_one_epoch(model_resunet, val_loader, criterion)

    train_loss_history.append(tr_loss)
    train_dice_history.append(tr_dice)
    train_iou_history.append(tr_iou)
    val_loss_history.append(va_loss)
    val_dice_history.append(va_dice)
    val_iou_history.append(va_iou)

    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"| Train Loss: {tr_loss:.4f}, Dice: {tr_dice:.4f}, IoU: {tr_iou:.4f} "
          f"| Val Loss: {va_loss:.4f}, Dice: {va_dice:.4f}, IoU: {va_iou:.4f}")

plt.figure(figsize=(14,6))

plt.subplot(2,3,1)
plt.plot(train_loss_history, label='Train Loss')
plt.plot(val_loss_history,   label='Val Loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(2,3,2)
plt.plot(train_dice_history, label='Train Dice')
plt.plot(val_dice_history,   label='Val Dice')
plt.title('Dice Coefficient')
plt.xlabel('Epoch')
plt.ylabel('Dice')
plt.legend()

plt.subplot(2,3,3)
plt.plot(train_iou_history, label='Train IoU')
plt.plot(val_iou_history,   label='Val IoU')
plt.title('IoU')
plt.xlabel('Epoch')
plt.ylabel('IoU')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate on test set
test_loss = 0.0
test_dice = 0.0
test_iou  = 0.0

model_resunet.eval()
with torch.no_grad():
    for images, masks in test_loader:
        outputs = model_resunet(images)
        loss = criterion(outputs, masks)

        d = dice_coeff(outputs, masks).item()
        i = iou_coeff(outputs, masks).item()

        test_loss += loss.item()
        test_dice += d
        test_iou  += i

        del images, masks, outputs, loss
        gc.collect()

n_test = len(test_loader)
test_loss /= n_test
test_dice /= n_test
test_iou  /= n_test

print(f"Test Loss: {test_loss:.4f}, Dice: {test_dice:.4f}, IoU: {test_iou:.4f}")

import random

n_samples = 3
indices = random.sample(range(len(test_dataset)), n_samples)

plt.figure(figsize=(12, 4*n_samples))

model_resunet.eval()
for i, idx in enumerate(indices):
    image_6, mask_gt = test_dataset[idx]  # shape => (6, H, W), (1, H, W)
    image_6_batch = image_6.unsqueeze(0)  # add batch dim

    with torch.no_grad():
        pred = model_resunet(image_6_batch)
    pred_sigmoid = torch.sigmoid(pred)
    pred_bin = (pred_sigmoid > 0.5).float()

    pred_np = pred_bin.squeeze().numpy()
    mask_np = mask_gt.squeeze().numpy()
    ndvi_np = image_6[5].numpy()  # NDVI is channel 5 if [0..4 => 5 bands]

    plt.subplot(n_samples, 3, i*3+1)
    plt.title("NDVI channel")
    plt.imshow(ndvi_np, cmap='RdYlGn')
    plt.axis('off')

    plt.subplot(n_samples, 3, i*3+2)
    plt.title("Pseudo Mask (GT)")
    plt.imshow(mask_np, cmap='gray')
    plt.axis('off')

    plt.subplot(n_samples, 3, i*3+3)
    plt.title("ResUNet Predicted")
    plt.imshow(pred_np, cmap='gray')
    plt.axis('off')

plt.tight_layout()
plt.show()