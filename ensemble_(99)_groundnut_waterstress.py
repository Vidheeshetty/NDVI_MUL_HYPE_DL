# -*- coding: utf-8 -*-
"""ensemble (99) GroundNut_Waterstress.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uzr174yuGVfI5b0UfuUxztqqDsBtDiGX
"""

# ===========================================================
# 1. (If in Colab) Mount Google Drive to access .npy files
# ===========================================================
from google.colab import drive
drive.mount('/content/drive')

# 1. Load the .npy files
# Adjust the file paths as needed for your environment
X = np.load('X_GN.npy')  # Hyperspectral data: shape could be (num_samples, num_bands)
Y = np.load('Y_GN.npy')  # Labels: shape could be (num_samples,)

import zipfile
import os

zip_path = "/content/drive/MyDrive/GROUNDNUT WATER STRESS DATA (1).zip"  # Replace with the path to your zip file

# Create a folder to extract
extract_dir = "/content/sample_data"
if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)

# Unzip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("Unzipping complete!")

import numpy as np

# Paths to the extracted .npy files
X_path = os.path.join(extract_dir, "/content/sample_data/GROUNDNUT WATER STRESS DATA/X_GN_31Dec.npy")
Y_path = os.path.join(extract_dir, "/content/sample_data/GROUNDNUT WATER STRESS DATA/y_GN_31Dec.npy")

# Load them
X = np.load(X_path)
Y = np.load(Y_path)

print("Shape of X:", X.shape)  # e.g. (num_samples, num_bands)
print("Shape of Y:", Y.shape)  # e.g. (num_samples,)

import matplotlib.pyplot as plt
import seaborn as sns

# Check label distribution
unique_labels, counts = np.unique(Y, return_counts=True)
print("Unique Labels in Y:", unique_labels)
print("Counts of each label:", counts)

# Plot class distribution
plt.figure(figsize=(6,4))
sns.countplot(x=Y, palette='viridis')
plt.title("Class Distribution")
plt.xlabel("Class Label")
plt.ylabel("Count")
plt.show()

# Plot a few random spectra
num_samples_to_plot = 5
indices = np.random.choice(range(X.shape[0]), num_samples_to_plot, replace=False)

plt.figure(figsize=(10,6))
for idx in indices:
    plt.plot(X[idx], label=f"Sample {idx} - Label {Y[idx]}")

plt.title("Random Hyperspectral Curves")
plt.xlabel("Band Index")
plt.ylabel("Reflectance")
plt.legend()
plt.show()

# ==============================================================================
# 2. LOAD DATA
# ==============================================================================
# If these .npy files are in the current working directory:
X = np.load("/content/sample_data/GROUNDNUT WATER STRESS DATA/X_GN_31Dec.npy")  # shape: (16667, 300)
Y = np.load("/content/sample_data/GROUNDNUT WATER STRESS DATA/y_GN_31Dec.npy")  # shape: (16667,)

print("Shape of X:", X.shape)  # e.g. (16667, 300)
print("Shape of Y:", Y.shape)  # e.g. (16667,)

# Check label distribution
unique_labels, counts = np.unique(Y, return_counts=True)
print("Unique Labels in Y:", unique_labels)
print("Counts of each label:", counts)

# Basic plot of class distribution
plt.figure(figsize=(6,4))
sns.countplot(x=Y, palette='viridis')
plt.title("Class Distribution (WW=1, WS=2)")
plt.xlabel("Class Label")
plt.ylabel("Count")
plt.show()

# ==============================================================================
# 3. CALCULATE NDVI (OPTIONAL)
#    Replace band indices with actual Red (~670 nm) and NIR (~800 nm)
# ==============================================================================
red_band_idx = 130  # placeholder for ~670 nm
nir_band_idx = 200  # placeholder for ~800 nm

red_vals = X[:, red_band_idx]
nir_vals = X[:, nir_band_idx]

# Avoid division by zero
ndvi = (nir_vals - red_vals) / (nir_vals + red_vals + 1e-10)
print(f"NDVI shape: {ndvi.shape}, NDVI range: [{ndvi.min()}, {ndvi.max()}]")

plt.figure(figsize=(8,5))
sns.histplot(ndvi, kde=True, bins=50, color='green')
plt.title("NDVI Distribution")
plt.xlabel("NDVI")
plt.ylabel("Count")
plt.show()

# Augment X with NDVI as an extra feature
X_augmented = np.concatenate([X, ndvi.reshape(-1, 1)], axis=1)
print("Shape of X_augmented:", X_augmented.shape)  # e.g. (16667, 301)

# ==============================================================================
# 4. OPTIONAL DATA AUGMENTATION
#    Adding Gaussian noise to each sample to increase robustness
# ==============================================================================
def add_spectral_noise(X_data, noise_level=0.01):
    """
    Adds Gaussian noise to each sample's spectrum.
    noise_level indicates the std of the Gaussian noise
    as a fraction of the mean reflectance.
    """
    noisy_data = []
    for i in range(X_data.shape[0]):
        sample = X_data[i]
        mean_val = np.mean(sample)
        noise = np.random.normal(0, noise_level * mean_val, sample.shape)
        noisy_data.append(sample + noise)
    return np.array(noisy_data)

# Create a noisy version
X_noisy = add_spectral_noise(X_augmented, noise_level=0.01)

# Combine original + noisy
X_final = np.concatenate((X_augmented, X_noisy), axis=0)
Y_final = np.concatenate((Y, Y), axis=0)

print("X_final shape after augmentation:", X_final.shape)
print("Y_final shape after augmentation:", Y_final.shape)

# If you do NOT want data augmentation, simply use:
# X_final = X_augmented
# Y_final = Y

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
import os

# ==============================================================================
# 5. TRAIN/TEST SPLIT
# ==============================================================================
X_train, X_test, Y_train, Y_test = train_test_split(
    X_final,
    Y_final,
    test_size=0.2,
    random_state=42,
    stratify=Y_final  # preserve class distribution
)

print("Training set size:", X_train.shape, Y_train.shape)
print("Test set size:", X_test.shape, Y_test.shape)

# ==============================================================================
# 6. SCALING
#    Standardize each feature (mean=0, std=1)
# ==============================================================================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

# ==============================================================================
# 7. PREP LABELS FOR BINARY CLASSIFICATION
#    Map 1 -> 0 (WW), 2 -> 1 (WS)
# ==============================================================================
Y_train_bin = (Y_train == 2).astype(np.float32)
Y_test_bin  = (Y_test == 2).astype(np.float32)

# ==============================================================================
# 8. RESHAPE FOR 1D CNN
#    1D CNN expects (samples, timesteps, channels)
# ==============================================================================
num_features = X_train_scaled.shape[1]  # 301 if NDVI appended
X_train_cnn = X_train_scaled.reshape(-1, num_features, 1)
X_test_cnn  = X_test_scaled.reshape(-1, num_features, 1)

print("X_train_cnn shape:", X_train_cnn.shape)
print("X_test_cnn shape:", X_test_cnn.shape)

# ==============================================================================
# 9. BUILD A FANCY 1D CNN
#    - Multiple Conv1D layers
#    - Batch Normalization
#    - Dropout
#    - EarlyStopping & ModelCheckpoint
# ==============================================================================
def create_fancy_1dcnn(input_shape):
    """
    A deeper 1D CNN for spectral data classification.
    """
    model = models.Sequential()

    # Conv Block 1
    model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu',
                            input_shape=input_shape))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2))
    model.add(layers.Dropout(0.2))

    # Conv Block 2
    model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2))
    model.add(layers.Dropout(0.2))

    # Conv Block 3
    model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2))
    model.add(layers.Dropout(0.2))

    # Flatten + Dense layers
    model.add(layers.Flatten())
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.3))

    model.add(layers.Dense(1, activation='sigmoid'))

    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    return model

# Instantiate the model
input_shape = (num_features, 1)  # e.g. (301, 1)
model = create_fancy_1dcnn(input_shape)
model.summary()

# Define callbacks
early_stop = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

checkpoint_path = "best_fancy_1dcnn.keras"
model_checkpoint = callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

# ==============================================================================
# 10. TRAIN THE MODEL
# ==============================================================================
history = model.fit(
    X_train_cnn,
    Y_train_bin,
    validation_split=0.2,
    epochs=30,
    batch_size=32,
    callbacks=[early_stop, model_checkpoint],
    verbose=1
)

# ==============================================================================
# 11. PLOT TRAINING CURVES
# ==============================================================================
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss vs. Epoch")
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy vs. Epoch")
plt.legend()

plt.tight_layout()
plt.show()

# ==============================================================================
# 12. EVALUATE ON TEST SET
# ==============================================================================
test_loss, test_acc = model.evaluate(X_test_cnn, Y_test_bin, verbose=0)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")

# ==============================================================================
# 13. PREDICTIONS AND METRICS
# ==============================================================================
Y_pred_prob = model.predict(X_test_cnn).ravel()  # probabilities for WS
Y_pred_bin = (Y_pred_prob > 0.5).astype(np.int32) # 0 or 1

# Map back to original labels (WW=1, WS=2)
Y_pred = Y_pred_bin + 1

# Confusion Matrix
cm = confusion_matrix(Y_test, Y_pred)
print("Confusion Matrix (Labels: 1=WW, 2=WS):\n", cm)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['WW (Pred)', 'WS (Pred)'],
            yticklabels=['WW (True)', 'WS (True)'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# Classification Report
print("\nClassification Report:")
print(classification_report(Y_test, Y_pred, digits=4))

# ==============================================================================
# 14. ROC CURVE
#    We'll treat label=2 (WS) as the positive class => Y_test_bin=1 => WS
# ==============================================================================
fpr, tpr, thresholds = roc_curve(Y_test_bin, Y_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, color='red', label=f"ROC Curve (AUC = {roc_auc:.3f})")
plt.plot([0,1], [0,1], color='blue', linestyle='--', label="Random")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

# ==============================================================================
# 15. ADDITIONAL PLOTS (MEAN SPECTRA, NDVI BY CLASS, ETC.)
# ==============================================================================
# Let's see average reflectance for WW vs WS
WW_indices = (Y == 1)
WS_indices = (Y == 2)

# X_augmented has the shape (n_samples, 301) => last column is NDVI
WW_spectrum_mean = np.mean(X_augmented[WW_indices, :-1], axis=0)  # ignoring NDVI column
WS_spectrum_mean = np.mean(X_augmented[WS_indices, :-1], axis=0)

plt.figure(figsize=(10,6))
plt.plot(WW_spectrum_mean, label="WW Mean Spectrum", color='green')
plt.plot(WS_spectrum_mean, label="WS Mean Spectrum", color='red')
plt.title("Mean Reflectance Spectrum (WW vs WS)")
plt.xlabel("Band Index")
plt.ylabel("Reflectance")
plt.legend()
plt.show()

# NDVI distribution per class
WW_ndvi = ndvi[WW_indices]
WS_ndvi = ndvi[WS_indices]

plt.figure(figsize=(8,5))
sns.histplot(WW_ndvi, color='g', label='WW', kde=True, stat='density')
sns.histplot(WS_ndvi, color='r', label='WS', kde=True, stat='density')
plt.title("NDVI Distribution by Class")
plt.xlabel("NDVI")
plt.ylabel("Density")
plt.legend()
plt.show()

"""# **1D TRANSFORMER **"""

# ==============================================================================
# 2. LOAD DATA
# ==============================================================================
X = np.load("/content/sample_data/GROUNDNUT WATER STRESS DATA/X_GN_31Dec.npy")  # shape: (16667, 300)
Y = np.load("/content/sample_data/GROUNDNUT WATER STRESS DATA/y_GN_31Dec.npy")  # shape: (16667,)

print("Shape of X:", X.shape)
print("Shape of Y:", Y.shape)

unique_labels, counts = np.unique(Y, return_counts=True)
print("Unique Labels in Y:", unique_labels)
print("Counts of each label:", counts)

plt.figure(figsize=(6,4))
sns.countplot(x=Y, palette='viridis')
plt.title("Class Distribution (WW=1, WS=2)")
plt.xlabel("Class Label")
plt.ylabel("Count")
plt.show()

# ==============================================================================
# 3. (OPTIONAL) NDVI CALCULATION
#    Replace these indices with actual Red (~670 nm) and NIR (~800 nm).
# ==============================================================================
red_band_idx = 130
nir_band_idx = 200

red_vals = X[:, red_band_idx]
nir_vals = X[:, nir_band_idx]

ndvi = (nir_vals - red_vals) / (nir_vals + red_vals + 1e-10)
print(f"NDVI shape: {ndvi.shape}, NDVI range: [{ndvi.min()}, {ndvi.max()}]")

plt.figure(figsize=(8,5))
sns.histplot(ndvi, kde=True, bins=50, color='green')
plt.title("NDVI Distribution")
plt.xlabel("NDVI")
plt.ylabel("Count")
plt.show()

# Append NDVI as an extra feature
X_augmented = np.concatenate([X, ndvi.reshape(-1, 1)], axis=1)
print("X_augmented shape:", X_augmented.shape)  # e.g. (16667, 301)

# ==============================================================================
# 4. (OPTIONAL) DATA AUGMENTATION
# ==============================================================================
def add_spectral_noise(X_data, noise_level=0.01):
    """
    Adds Gaussian noise to each sample's spectrum.
    noise_level indicates the std of the Gaussian noise
    as a fraction of the mean reflectance.
    """
    noisy_data = []
    for i in range(X_data.shape[0]):
        sample = X_data[i]
        mean_val = np.mean(sample)
        noise = np.random.normal(0, noise_level * mean_val, sample.shape)
        noisy_data.append(sample + noise)
    return np.array(noisy_data)

# Create a noisy version
X_noisy = add_spectral_noise(X_augmented, noise_level=0.01)

# Combine original + noisy
X_final = np.concatenate((X_augmented, X_noisy), axis=0)
Y_final = np.concatenate((Y, Y), axis=0)

print("X_final shape after augmentation:", X_final.shape)  # (33334, 301)
print("Y_final shape after augmentation:", Y_final.shape)  # (33334,)

# If you don't want augmentation, just do:
# X_final = X_augmented
# Y_final = Y

# ==============================================================================
# 5. TRAIN/TEST SPLIT
# ==============================================================================
X_train, X_test, Y_train, Y_test = train_test_split(
    X_final,
    Y_final,
    test_size=0.2,
    random_state=42,
    stratify=Y_final
)

print("X_train:", X_train.shape, "Y_train:", Y_train.shape)
print("X_test:", X_test.shape, "Y_test:", Y_test.shape)

# ==============================================================================
# 6. SCALING
# ==============================================================================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ==============================================================================
# 7. PREP LABELS FOR BINARY CLASSIFICATION
#    Map 1 -> 0 (WW), 2 -> 1 (WS)
# ==============================================================================
Y_train_bin = (Y_train == 2).astype(np.float32)
Y_test_bin  = (Y_test == 2).astype(np.float32)

# ==============================================================================
# 8. RESHAPE FOR A 1D TRANSFORMER
#    We'll treat each sample as a sequence of "num_features" steps (301),
#    and each step has 1 channel.
# ==============================================================================
num_features = X_train_scaled.shape[1]  # e.g., 301
X_train_seq = X_train_scaled.reshape(-1, num_features, 1)
X_test_seq  = X_test_scaled.reshape(-1, num_features, 1)

print("X_train_seq shape:", X_train_seq.shape)
print("X_test_seq shape: ", X_test_seq.shape)

# ==============================================================================
# 9. DEFINE A 1D TRANSFORMER BLOCK
# ==============================================================================
def transformer_encoder(inputs, num_heads, key_dim, ff_dim, rate=0.1):
    """
    A single Transformer encoder block:
    - Multi-Head Self-Attention
    - Skip connection + Layer Normalization
    - Feed Forward (Dense) + skip connection + Layer Normalization
    """
    # Layer Normalization 1
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    # Multi-Head Attention
    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)
    # Skip Connection
    x = layers.Add()([x, inputs])

    # Layer Normalization 2
    y = layers.LayerNormalization(epsilon=1e-6)(x)
    # Feed-Forward
    y = layers.Dense(ff_dim, activation='relu')(y)
    y = layers.Dropout(rate)(y)
    y = layers.Dense(key_dim, activation='relu')(y)
    # Skip Connection
    return layers.Add()([x, y])

def create_spectral_transformer(input_shape,
                                num_transformer_blocks=3,
                                embed_dim=64,
                                num_heads=4,
                                ff_dim=128,
                                rate=0.1):
    """
    A deeper 1D Transformer model for spectral data.
    - input_shape: (sequence_length, 1)
    - num_transformer_blocks: how many stacked transformer encoder blocks
    - embed_dim: dimension for the projected embeddings
    - num_heads: number of attention heads
    - ff_dim: hidden dimension in feed-forward networks inside each block
    - rate: dropout rate
    """
    inputs = layers.Input(shape=input_shape)  # e.g. (301, 1)

    # 1) Project up from 1 channel to 'embed_dim' channels using a 1D conv
    x = layers.Conv1D(filters=embed_dim, kernel_size=1, activation='relu')(inputs)

    # 2) Stack multiple Transformer blocks
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, num_heads=num_heads, key_dim=embed_dim, ff_dim=ff_dim, rate=rate)

    # 3) Global pooling
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dropout(rate)(x)

    # 4) Dense layers
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(rate)(x)

    # 5) Output
    outputs = layers.Dense(1, activation='sigmoid')(x)

    # Compile model
    model = models.Model(inputs=inputs, outputs=outputs)
    model.compile(
        optimizer=optimizers.Adam(learning_rate=1e-3),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model

# Build the spectral transformer
model = create_spectral_transformer(
    input_shape=(num_features, 1),  # e.g., (301,1)
    num_transformer_blocks=3,      # increase for deeper model
    embed_dim=64,                  # embedding dimension
    num_heads=4,                   # number of attention heads
    ff_dim=128,                    # feed-forward dimension
    rate=0.2                       # dropout rate
)

model.summary()

# ==============================================================================
# 10. CALLBACKS (EarlyStopping, ModelCheckpoint)
# ==============================================================================
early_stop = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

checkpoint_path = "best_spectral_transformer.keras"
model_checkpoint = callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

# ==============================================================================
# 11. TRAIN THE MODEL
# ==============================================================================
history = model.fit(
    X_train_seq,
    Y_train_bin,
    validation_split=0.2,
    epochs=30,
    batch_size=32,
    callbacks=[early_stop, model_checkpoint],
    verbose=1
)

# ==============================================================================
# 12. PLOT TRAINING CURVES
# ==============================================================================
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss vs. Epoch")
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy vs. Epoch")
plt.legend()

plt.tight_layout()
plt.show()

# ==============================================================================
# 13. EVALUATE ON TEST SET
# ==============================================================================
test_loss, test_acc = model.evaluate(X_test_seq, Y_test_bin, verbose=0)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")

# ==============================================================================
# 14. PREDICTIONS & METRICS
# ==============================================================================
Y_pred_proba = model.predict(X_test_seq).ravel()  # probability for WS class
Y_pred_bin = (Y_pred_proba > 0.5).astype(int)
Y_pred = Y_pred_bin + 1  # map 0->1 (WW), 1->2 (WS)

cm = confusion_matrix(Y_test, Y_pred)
print("Confusion Matrix (1=WW, 2=WS):\n", cm)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['WW (Pred)', 'WS (Pred)'],
            yticklabels=['WW (True)', 'WS (True)'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

print("\nClassification Report:")
print(classification_report(Y_test, Y_pred, digits=4))

# ROC Curve (WS = positive class)
fpr, tpr, thresholds = roc_curve(Y_test_bin, Y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, color='red', label=f"ROC Curve (AUC={roc_auc:.3f})")
plt.plot([0,1],[0,1], color='blue', linestyle='--', label="Random")
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

# ==============================================================================
# 15. OPTIONAL: ADDITIONAL PLOTS
# ==============================================================================
# Mean reflectance for WW vs WS
WW_indices = (Y == 1)
WS_indices = (Y == 2)

WW_spectrum_mean = np.mean(X_augmented[WW_indices, :-1], axis=0)  # ignoring NDVI col
WS_spectrum_mean = np.mean(X_augmented[WS_indices, :-1], axis=0)

plt.figure(figsize=(10,6))
plt.plot(WW_spectrum_mean, label="WW Mean Spectrum", color='g')
plt.plot(WS_spectrum_mean, label="WS Mean Spectrum", color='r')
plt.xlabel("Band Index")
plt.ylabel("Reflectance")
plt.title("Mean Reflectance Spectrum (WW vs WS)")
plt.legend()
plt.show()

# NDVI distribution by class
WW_ndvi = ndvi[WW_indices]
WS_ndvi = ndvi[WS_indices]

plt.figure(figsize=(8,5))
sns.histplot(WW_ndvi, color='g', label='WW', kde=True, stat='density')
sns.histplot(WS_ndvi, color='r', label='WS', kde=True, stat='density')
plt.title("NDVI Distribution by Class")
plt.xlabel("NDVI")
plt.ylabel("Density")
plt.legend()
plt.show()

"""# **ML (Random Forest & SVM ) **"""

import numpy as np

# Load the .npy files
X = np.load("/content/sample_data/GROUNDNUT WATER STRESS DATA/X_GN_31Dec.npy")  # shape: (16667, 300) for example
Y = np.load("/content/sample_data/GROUNDNUT WATER STRESS DATA/y_GN_31Dec.npy")  # shape: (16667,)

# 1. Print shapes
print("X shape:", X.shape)
print("Y shape:", Y.shape)

# 2. Print the first 5 samples in X (first 10 bands only)
#    (Feel free to adjust the slicing to see more or fewer bands.)
print("\n===== A Peek at X (First 5 samples, first 10 bands) =====")
print(X[:5, :10])

# 3. Print the first 10 labels in Y
print("\n===== A Peek at Y (First 10 labels) =====")
print(Y[:10])

# 4. Print the final 5 samples in X (last 10 bands)
print("\n===== A Peek at X (Last 5 samples, last 10 bands) =====")
print(X[-5:, -10:])

# 5. Print some random samples from X and Y
import numpy as np
np.random.seed(42)
random_indices = np.random.choice(range(X.shape[0]), size=3, replace=False)

print("\n===== Some Random Samples =====")
for idx in random_indices:
    print(f"Sample Index: {idx}")
    print("  X (first 5 bands):", X[idx, :5])
    print("  Y (label)        :", Y[idx])

# 3. PLOT CLASS DISTRIBUTION
plt.figure(figsize=(6,4))
sns.countplot(x=Y, palette='viridis')
plt.title("Class Distribution (WW=1, WS=2)")
plt.xlabel("Class Label")
plt.ylabel("Count")
plt.show()

# 4. PLOT RANDOM SAMPLE SPECTRA
#    We'll pick 5 random indices and plot their reflectance across all bands
num_to_plot = 5
indices = np.random.choice(range(X.shape[0]), num_to_plot, replace=False)

plt.figure(figsize=(10,6))
for idx in indices:
    plt.plot(X[idx], label=f"Sample {idx} (Label={Y[idx]})")
plt.title("Random Hyperspectral Spectra")
plt.xlabel("Band Index")
plt.ylabel("Reflectance")
plt.legend()
plt.show()

# 5. PLOT MEAN SPECTRUM FOR EACH CLASS
#    Let's assume we have 2 classes: 1=Well-Watered, 2=Water-Stressed
WW_indices = (Y == 1)
WS_indices = (Y == 2)

WW_mean_spectrum = np.mean(X[WW_indices], axis=0)
WS_mean_spectrum = np.mean(X[WS_indices], axis=0)

plt.figure(figsize=(10,6))
plt.plot(WW_mean_spectrum, label="WW Mean Spectrum", color='green')
plt.plot(WS_mean_spectrum, label="WS Mean Spectrum", color='red')
plt.title("Mean Reflectance Spectrum for WW vs. WS")
plt.xlabel("Band Index")
plt.ylabel("Reflectance")
plt.legend()
plt.show()

# 6. (OPTIONAL) PLOT A HISTOGRAM OF VALUES IN A SPECIFIC BAND
#    e.g., band 130 ~ Red (approx 670 nm)
red_band_idx = 130  # adjust if needed
red_vals = X[:, red_band_idx]

plt.figure(figsize=(8,5))
sns.histplot(red_vals, bins=50, kde=True, color='maroon')
plt.title(f"Distribution of Reflectance in Band {red_band_idx}")
plt.xlabel("Reflectance")
plt.ylabel("Count")
plt.show()

# 7. PRINT SOME RANDOM SAMPLES WITH PART OF THEIR REFLECTANCE
np.random.seed(42)
random_indices = np.random.choice(range(X.shape[0]), size=3, replace=False)
print("\n===== RANDOM SAMPLES (showing first 5 band values) =====")
for idx in random_indices:
    print(f"Sample {idx} - Label: {Y[idx]}")
    print("  First 5 band values:", X[idx, :5])
    print("  ...")

# ==============================================================================
# 1. IMPORT LIBRARIES
# ==============================================================================
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc

# ML models
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from xgboost import XGBClassifier  # optional: conda install -c conda-forge xgboost

# ==============================================================================
# 3. PREPROCESSING: NDVI CALCULATION (OPTIONAL)
#    Replace red_band_idx and nir_band_idx with correct indices for your sensor
# ==============================================================================
red_band_idx = 130  # placeholder for ~670 nm
nir_band_idx = 200  # placeholder for ~800 nm

red_vals = X[:, red_band_idx]
nir_vals = X[:, nir_band_idx]

# NDVI
ndvi = (nir_vals - red_vals) / (nir_vals + red_vals + 1e-10)
print("NDVI shape:", ndvi.shape, "Range:", ndvi.min(), "to", ndvi.max())

# Plot NDVI distribution
plt.figure(figsize=(8,5))
sns.histplot(ndvi, kde=True, bins=50, color='green')
plt.title("NDVI Distribution")
plt.xlabel("NDVI")
plt.ylabel("Count")
plt.show()

# Append NDVI as an extra feature
X_augmented = np.concatenate([X, ndvi.reshape(-1, 1)], axis=1)
print("Shape of X_augmented:", X_augmented.shape)  # e.g., (16667, 301)

# ==============================================================================
# 4. OPTIONAL DATA AUGMENTATION
#    Adding small Gaussian noise to the augmented feature matrix
# ==============================================================================
def add_spectral_noise(X_data, noise_level=0.01):
    """
    Adds Gaussian noise to each sample's features.
    noise_level is the std of noise as a fraction of the mean reflectance.
    """
    noisy_data = []
    for i in range(X_data.shape[0]):
        sample = X_data[i]
        mean_val = np.mean(sample)
        noise = np.random.normal(0, noise_level * mean_val, sample.shape)
        noisy_data.append(sample + noise)
    return np.array(noisy_data)

# Create a noisy version
X_noisy = add_spectral_noise(X_augmented, noise_level=0.01)
# Combine original + noisy
X_final = np.concatenate((X_augmented, X_noisy), axis=0)
Y_final = np.concatenate((Y, Y), axis=0)

print("X_final shape after augmentation:", X_final.shape)
print("Y_final shape after augmentation:", Y_final.shape)

# If you DON'T want augmentation, simply do:
# X_final = X_augmented
# Y_final = Y

# ==============================================================================
# 5. TRAIN/TEST SPLIT
# ==============================================================================
X_train, X_test, Y_train, Y_test = train_test_split(
    X_final,
    Y_final,
    test_size=0.2,
    random_state=42,
    stratify=Y_final
)
print("X_train shape:", X_train.shape, "Y_train shape:", Y_train.shape)
print("X_test shape: ", X_test.shape,  "Y_test shape: ", Y_test.shape)

# ==============================================================================
# 6. SCALING
#    Standardize features (mean=0, std=1)
# ==============================================================================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

# ==============================================================================
# 7. BUILD / TUNE ML MODELS
#    We'll create:
#      - Random Forest
#      - Gradient Boosting (via XGB or sklearn)
#      - Then combine them with VotingClassifier
#    For highest accuracy, we do a bit of hyperparam tuning on each.
# ==============================================================================

# --- 7.1 Random Forest (basic or with GridSearchCV) ---
rf_clf = RandomForestClassifier(
    n_estimators=200,
    max_depth=20,
    random_state=42
)
rf_clf.fit(X_train_scaled, Y_train)
rf_acc = accuracy_score(Y_test, rf_clf.predict(X_test_scaled))
print("RF Accuracy:", rf_acc)

# Suppose Y has values 1 (WW) and 2 (WS)
# Map them to 0 and 1
Y_train_bin = Y_train - 1  # Now 0 = (WW), 1 = (WS)
Y_test_bin  = Y_test - 1

from xgboost import XGBClassifier

xgb_clf = XGBClassifier(
    objective='binary:logistic',  # For binary classification
    eval_metric='logloss',
    use_label_encoder=False,
    n_estimators=200,
    max_depth=8,
    learning_rate=0.1,
    random_state=42
)

# Train on X_train_scaled, Y_train_bin (0 or 1)
xgb_clf.fit(X_train_scaled, Y_train_bin)

# Predictions → [0, 1]
y_pred_bin = xgb_clf.predict(X_test_scaled)

# Map [0,1] back to [1,2] if you need original labels
y_pred = y_pred_bin + 1



# --- 7.3 Gradient Boosting (SKLearn) [Alternative to XGBoost] ---
gb_clf = GradientBoostingClassifier(
n_estimators=200,
max_depth=5,
learning_rate=0.1,
random_state=42
 )
gb_clf.fit(X_train_scaled, Y_train)
gb_acc = accuracy_score(Y_test, gb_clf.predict(X_test_scaled))
print("GB Accuracy:", gb_acc)

voting_clf = VotingClassifier(
    estimators=[
        ('rf', rf_clf),
        ('xgb', xgb_clf),
        ('gb', gb_clf),  # if you want to include a third
    ],
    voting='soft'  # 'hard' or 'soft'. 'soft' uses predicted probabilities
)
voting_clf.fit(X_train_scaled, Y_train)

ensemble_pred = voting_clf.predict(X_test_scaled)
ensemble_acc = accuracy_score(Y_test, ensemble_pred)
print("Ensemble Accuracy:", ensemble_acc)

pip install --upgrade xgboost scikit-learn==1.2.2

print("\nFinal Model Accuracy:", ensemble_acc)

cm = confusion_matrix(Y_test, ensemble_pred)
print("Confusion Matrix (1=WW, 2=WS):\n", cm)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['WW (Pred)', 'WS (Pred)'],
            yticklabels=['WW (True)', 'WS (True)'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print("\nClassification Report:")
print(classification_report(Y_test, ensemble_pred, digits=4))

# ROC Curve (for binary classification: label=2 => positive)
# We'll get the probability of class "2" from the voting classifier
proba = voting_clf.predict_proba(X_test_scaled)[:,1]
Y_test_bin = (Y_test == 2).astype(int)

fpr, tpr, thresholds = roc_curve(Y_test_bin, proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, color='red', label=f"ROC (AUC={roc_auc:.3f})")
plt.plot([0,1],[0,1], color='blue', linestyle='--', label="Random")
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

# ==============================================================================
# 10. ADDITIONAL VISUALIZATIONS (Mean Spectra, NDVI Distributions, etc.)
# ==============================================================================
WW_indices = (Y == 1)
WS_indices = (Y == 2)

# Average spectrum for each class (original X, ignoring last NDVI col)
WW_mean_spectrum = np.mean(X[WW_indices], axis=0)
WS_mean_spectrum = np.mean(X[WS_indices], axis=0)

plt.figure(figsize=(10,6))
plt.plot(WW_mean_spectrum, label="WW Mean Spectrum", color='green')
plt.plot(WS_mean_spectrum, label="WS Mean Spectrum", color='red')
plt.title("Mean Reflectance Spectrum (WW vs WS)")
plt.xlabel("Band Index")
plt.ylabel("Reflectance")
plt.legend()
plt.show()

# NDVI distributions
WW_ndvi = ndvi[WW_indices]
WS_ndvi = ndvi[WS_indices]

plt.figure(figsize=(8,5))
sns.histplot(WW_ndvi, color='g', label='WW', kde=True, stat='density')
sns.histplot(WS_ndvi, color='r', label='WS', kde=True, stat='density')
plt.title("NDVI Distribution by Class")
plt.xlabel("NDVI")
plt.ylabel("Density")
plt.legend()
plt.show()

import pandas as pd

# We'll create a small DataFrame to show side-by-side predictions
# Let's pick 10 random indices from the test set
np.random.seed(42)
sample_indices = np.random.choice(range(len(X_test_scaled)), size=10, replace=False)

pred_probs = voting_clf.predict_proba(X_test_scaled)[:, 1]  # Probability of class "2"
Y_pred_test = voting_clf.predict(X_test_scaled)

rows = []
for idx in sample_indices:
    true_label = Y_test[idx]
    predicted_label = Y_pred_test[idx]
    prob_ws = pred_probs[idx]
    rows.append({
        "SampleIndex": idx,
        "TrueLabel": true_label,
        "PredLabel": predicted_label,
        "Prob_WS": f"{prob_ws:.3f}"
    })

df_samples = pd.DataFrame(rows)
print(df_samples.sort_values("SampleIndex"))

# Retrieve feature importances from the RandomForest sub-model
importances = rf_clf.feature_importances_

# Let's create names for the 301 features: Band0 ... Band299, NDVI
num_original_bands = 300  # known from X
feature_names = [f"Band{i}" for i in range(num_original_bands)] + ["NDVI"]

# Pair each feature with its importance
feat_imp_pairs = list(zip(feature_names, importances))
# Sort by importance descending
feat_imp_pairs.sort(key=lambda x: x[1], reverse=True)

# Take top 20
top_k = 20
top_features = feat_imp_pairs[:top_k]

# Separate into two lists for plotting
top_feat_names  = [x[0] for x in top_features]
top_feat_values = [x[1] for x in top_features]

plt.figure(figsize=(8,6))
sns.barplot(x=top_feat_values, y=top_feat_names, palette='viridis')
plt.title(f"Top {top_k} Feature Importances (Random Forest)")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

from sklearn.inspection import PartialDependenceDisplay

# NDVI is at index 300 in X_augmented
ndvi_feature_index = 300

# Plot partial dependence for the WS class
# Note: The RF was trained on labels [1,2].
# Internally, scikit-learn might interpret them as [0,1]. We'll check the "target=1" for probability of class "1"
# Usually, "1" is the second class in alphabetical order if the classes are [1,2].
# If partial_dependence complains, you might need 'target=1'.
try:
    PartialDependenceDisplay.from_estimator(
        estimator=rf_clf,
        X=X_train_scaled,         # training data (scaled)
        features=[ndvi_feature_index],
        target=2,                 # or 'target=1' if classes are [1,2] => index 1
        grid_resolution=50
    )
except ValueError as e:
    print("Trying with target=1 due to label encoding:", e)
    PartialDependenceDisplay.from_estimator(
        estimator=rf_clf,
        X=X_train_scaled,
        features=[ndvi_feature_index],
        target=1,  # index of class '2'
        grid_resolution=50
    )

plt.title("Partial Dependence of RandomForest on NDVI")
plt.xlabel("NDVI (Scaled)")
plt.ylabel("Predicted Probability (WS)")
plt.show()

# 1) We'll get the NDVI column from X_test before scaling (the last col if augmented).
#    We can re-check NDVI from the *unscaled* data to print actual NDVI values.
test_ndvi = X_test[:, -1]  # NDVI is last column in X_final (unscaled)

# 2) Predict probabilities for water stress
prob_ws = voting_clf.predict_proba(X_test_scaled)[:, 1]  # Probability of class "2"

# 3) Get predicted labels
pred_labels = voting_clf.predict(X_test_scaled)

# 4) Let's examine 10 random test samples
np.random.seed(42)
sample_indices = np.random.choice(range(len(X_test)), size=10, replace=False)

rows = []
for idx in sample_indices:
    rows.append({
        "TestIndex": idx,
        "NDVI": f"{test_ndvi[idx]:.3f}",
        "Prob_WS": f"{prob_ws[idx]:.3f}",
        "PredLabel": int(pred_labels[idx]),
        "TrueLabel": int(Y_test[idx])
    })

df_test_samples = pd.DataFrame(rows).sort_values("TestIndex")
print("\n===== SAMPLE TEST DATA (NDVI, PredProb, PredLabel, TrueLabel) =====")
print(df_test_samples)

WW_indices_test = (Y_test == 1)
WS_indices_test = (Y_test == 2)

WW_ndvi_test = test_ndvi[WW_indices_test]
WS_ndvi_test = test_ndvi[WS_indices_test]

print(f"\nMean NDVI for WW in Test: {WW_ndvi_test.mean():.3f}")
print(f"Mean NDVI for WS in Test: {WS_ndvi_test.mean():.3f}")

plt.figure(figsize=(8,5))
sns.kdeplot(WW_ndvi_test, color='green', label='WW NDVI', fill=True)
sns.kdeplot(WS_ndvi_test, color='red', label='WS NDVI', fill=True)
plt.title("NDVI Distribution for Test Samples by True Label")
plt.xlabel("NDVI")
plt.ylabel("Density")
plt.legend()
plt.show()

# Choose 5 random samples from the test set
np.random.seed(123)
example_indices = np.random.choice(range(len(X_test)), 5, replace=False)

rows = []
for i in example_indices:
    # Remember: the original X_test (unscaled) has shape (#samples, 301) if NDVI appended
    sample_data = X_test[i]
    red_val  = sample_data[red_band_idx]
    nir_val  = sample_data[nir_band_idx]
    ndvi_val = sample_data[-1]  # last col is NDVI

    prob_ws_i = prob_ws[i]
    pred_label_i = pred_labels[i]
    true_label_i = Y_test[i]

    rows.append({
        "TestIndex": i,
        "RedBandVal (~670nm)": f"{red_val:.3f}",
        "NIRBandVal (~800nm)": f"{nir_val:.3f}",
        "NDVI": f"{ndvi_val:.3f}",
        "Prob_WS": f"{prob_ws_i:.3f}",
        "PredLabel": int(pred_label_i),
        "TrueLabel": int(true_label_i),
    })

df_bands = pd.DataFrame(rows).sort_values("TestIndex")
print("\n===== SAMPLE SPECTRAL VALUES & NDVI =====")
print(df_bands)

